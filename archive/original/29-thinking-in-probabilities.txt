Speaker 1 (00:00):
I can really empathize with that because even in my own sort of maths degree, I really struggled with terminology and notation. And I think a big problem in kind of maths education generally is that there's a lot of focus on notation and terminology. And you kind of miss the forest for the trees. 

Speaker 2 (00:25):
Hello, and welcome to Metta muse, muse as a tool for thought on iPad. This podcast, isn't about muse the product it's about muse the company and the small team behind it. I'm Adam Wiggins joined today by my colleague mark McGranahan Adam and our guest Tamar Abdol. 

Speaker 1 (00:43):
Hey guys, how's it going? Container? I understand you're enjoying London Springs so far. Yeah. I actually went outdoors for the first time and I don't know, six months or something. Yeah. I forgotten just how nice it is. Sit on the grass in the sun. Just chatting with friends about nothing in particular. Yeah. It was amazing what an experience. Yeah. The ability to go out and enjoy. We've had sort of triple threat here in my household because we've had one lockdown which has been pretty severe of course, for the last six months or so too. We had a pretty serious winter. In fact, it was snowing today and three, I've got a, a young child at home. So all of those things mean that I basically barely leave the house happily. I do have a dog, so I have to go out for walks on that. 

Speaker 1 (01:29):
If it wasn't for that, I would never see the outside, I think. Yeah, that's pretty rough, well time. Or maybe you can tell our audience a little bit about your background, including your podcast and the product you're working on now. Awesome. So I'm Tamar, I'm one of the co-founders of a company called causal. We're building a spreadsheets just for number crunching. So anything involving numbers, we want causal to be the way to do that. On the site. I have a podcast with my brother where we just catch up once a week and chat about what it was on our mind. And my background is mostly in maths. So I studied maths university specialized in statistics and machine learning and that kind of stuff. And what sorts of things do people use your product for? Is this a total replacement for spreadsheet or just a subset of that? 

Speaker 1 (02:13):
Yes, it's pretty just a subset of that a week, sort of think of spreadsheets as something like causal our products plus something like add table. So as Abel is kind of taking over all the non-numerical stuff you might do in a spreadsheet. So making lists of things, managing processes, you know, internal tools and that kind of stuff. And we won't call it abuse for anything enrolled with numbers. So anytime you to sort of write for ministers that do calculations or visualize data, that kind of stuff is really what causes it is about. And I certainly think that one of the main uses for spreadsheets for me and my business life, I guess, as well as helping others is this modeling often financial modeling where you're just trying to understand, because of course money is the lifeblood of a business, both how you earn money, because that proves you're providing value to people as well as just not printing out the money in the bank so that your business doesn't die and spreadsheets as a, what if tool to understand both what might happen in the future, but in many cases, just the viability of your business model. 

Speaker 1 (03:12):
One example I remember is I had a friend who was starting a retro kind of 1980s arcade, and they really wanted to run the games off quarters because that gives that authentic eighties feel. And then I'm saying, well, okay, but if you look at the inflation since the 1980s a quarter, isn't what it used to be. This is a us dollar quarter, of course. So we actually modeled all that out and plugged in a bunch of what if values and basically figured out that under no reasonable, we're just taking guesstimates for how many games now, or someone's going to play, how long they're going to spend in the arcade, that sort of thing, but basically nothing. We modeled show that it would be viable to stay in business with all the costs. And eventually did settle on a model, which was more like a flat rate. 

Speaker 1 (03:54):
You pay 10 bucks or 12 bucks or something when you come in the front door, which ends up both feeling maybe more fair and more fun for the patrons, but also is actually viable. I think it's maybe an example of where having ranges, which we were talking about a little bit earlier where you don't necessarily know what exactly each patron is going to spend on drinks or quarters are going to put in, or how many games are playing out or whatever, but you can plug in reasonable ranges. And from that you can infer maybe it ends up being like a Drake's equation kind of thing in that scenario where you can figure out what's viable and what isn't. Yeah. I think this idea of ranges is really powerful and I sort of personally, whenever I'm giving an estimate for something, I get really anxious that my estimate is going to be wrong. 

Speaker 1 (04:37):
And it just gives me a lot of comfort in providing a range because then I know that it's probably right rather than sort of precisely wrong. And so I find that even when, just sort of communicating days a day, if someone else would be for an estimate of something, if I give a single number, then for the next five minutes, I'll be like thinking through it in my head of like, oh, maybe that was wrong. Whereas if I bought a range, like I think it's between five and 10, then I sort of have the peace of mind of knowing that I haven't sort of been too inaccurate, I guess. And I think that's also a way to train yourself, to give estimates. I've run into this with a lot of folks who exactly, as you said, don't feel comfortable giving an estimate. Cause I feel like, well, I don't know, but you can always kind of start with, okay, you know, can you guess the price of product X in a supermarket? 

Speaker 1 (05:24):
Or can you guess the weight of this? And maybe you can't do that or you feel like you don't, but you can come up with a number that is so low, that is clearly outside the bottom of the range. And you come up with another number that's clearly so high it's outside of the top of the range. All right. So now you're working on it now let's narrow this window in. Yeah. That's one of my favorite tactics is a strong word, but one of my favorite things is if I'm like talking to a friend and yeah, exactly. Like you described, I think if you ask someone to try and quantify something that they're not used to quantifying, then they'll probably just say, oh, I didn't know. I could possibly put a number on that. But then if you ask, well, is it more than 10? 

Speaker 1 (05:59):
Is it less than 500? You know, you can actually get some pretty good range. And it's actually helpful to know that range rather than just put your hands up and say that it's unquantifiable. Yeah. Well maybe that brings us to our topic for today, which is thinking in probabilities. And I thought it was really interesting that you mentioned this as kind of a founding idea for you and then maybe in some ways you moved away from it in the product or maybe just in the marketing, but tell us what it means to think in probabilities. Yeah, absolutely. So the sort of origin story for causal, it kind of comes from some work I did as a data scientist in a previous job, I was working for a property tech company where essentially the company was placing big bets on houses. And so in typical fashion, we had a bunch of spreadsheet, financial models that would forecast the company's cashflow and some pretty big decisions were made on the back of these models. 

Speaker 1 (06:51):
Like how many deals can we do every month? How many people can we hire someone? And one of the really important things for this company was understanding the risk that we were taking on each deal. If we were placing a big bet on a house, the house might be worth a lot more than what we thought it'd be worth, or it might be worth a lot less and actually understanding how those would affect our bottom line was really important. And, you know, in spreadsheets, Google sheets, in this instance, we had to do a bunch of work arounds and hacks to try and get at this idea of essentially a probability distribution for how much a house would be worth. And there's various kind of ways to try and approximate that in a spreadsheet, but essentially trying to sort of get at this idea of probability added so much complexity to these spreadsheets that they became unmaintainable and no one really understood how they worked. 

Speaker 1 (07:32):
It was very hard to actually iterate on them. And so that was kind of my first exposure to this problem of how do you crunch numbers when some of them are uncertain, how do you build probabilistic models to try and understand the world and our starting point for causal and sort of our original mission was kind of to bring probabilities to the masses, to build a tool that makes it so easy to work with probability and uncertainty and so on. They become sort of the standard way that people sort of think numerically does that kind of make sense? Yeah. To me, it leads into the question of how much is it a tools gap that the average intelligent, educated, let's say knowledge worker that has a reason to want to be able to think in probabilities or model uncertainty, numerically, how much is it that the tools make it tricky like you described with spreadsheets and how much is it more, a matter of it's very hard for humans to think this way, even intelligent, educated people, it doesn't come naturally unless you've studied math or made this your career or your passion in life that you've sort of struggled to apply this approach. 

Speaker 1 (08:39):
Yeah, absolutely. I think it's a really good question. And it's hard to know which side leads to which an example that I often think of is this idea of having a line of best fit for some data set as quite common, even sort of newspapers magazines to see like a 2d chops with a bunch of data points and then some kind of straight line drawn through these data points to kind of extrapolate some kind of trend and tell some kind of story. And if we think about what does that actually mean? I think most people, if they look at a graph like that, they will understand immediately what the graph is trying to say. The brothels typically be fine to say that as this one thing increases, this other thing increases as well or as this one thing increases and something decreases without a particularly map, see backgrounds, you can read a child like that and you understand what's going on. 

Speaker 1 (09:24):
I think the really cool thing about the line of best fits that is now just so super common and everyone gets it is that very few people, unless you sort of studied math, so maybe computer science, very few people will be able to tell you how you derive that line of best fit. And the best part is they don't need to be able to tell you that there need to know that behind the scenes, you have to inverse a matrix in order to like figure out this line or anything like that. And I think in that sense, just visualizing something in the right way is kind of a powerful tool to unlock intuition that we already had. And so in the example of line of best fit, I can describe to you some effects, like as you get closer to the center of London, property prices go up, you know, I can describe that to you. 

Speaker 1 (10:02):
You understand what that means in your head. And if I showed that to you on a chart, you'd immediately kind of get what I'm trying to communicate. And so I think the probability stuff might be similar where so far we haven't had the line of best fit moment for probability. We haven't found a sort of killer tool or killer sort of visualization that anyone can sort of look at and understand. I do think this is just really unintuitive in general as well, but again, it's hard to say whether it's unintuitive because we haven't had some really basic tools, like just being able to visualize it or whether it's sort of inherently on intuitive for humans. So I studied a lot of probability and statistics in my degree. And so after graduating, I kind of felt like I had a good handle of this stuff, but it was off to actually facing a lot of these problems in moving. 

Speaker 1 (10:48):
How do you account for uncertainty and models and things like that? I kind of realized that studying the theory of probability and, you know, being able to prove certain theater rooms and things like that is actually almost a completely separate tasks from having the right intuition about these things. And so I think there's a really common example that Nassim Taleb is a big fan of which is that you wouldn't want to cross a river. That is four feet deep on average. And yeah, obviously if it's four feet deep on average, it might be eight 50 deep in one particular parts. And you might drown as I think he often talks about the dangers of working with averages. I think another kind of industry specific example is sort of to do with a buffet. If you imagine, you know, you're putting together a buffet and there's 10 dishes in the buffet and each dish takes on average about an hour to prepare. 

Speaker 1 (11:31):
And the whole buffet is ready once all 10 dishes are ready. So each dish has an average time of one hour. And if you were trying to think about, you know, what is the average time for the whole buffet to be ready? It's tempting to think that each dishes ready in an hour on average. And so the whole buffet will be ready in an hour on average, but, and the two, the two answers, he would jump to two there as either 10 hours because of sequential or one hour, because it's all parallel. Yeah, exactly. So actually even in the parallel case, it turns out that the average time for the buffet to be ready is actually a lot more than one hour. And this is sort of like the most basic example of where average outcomes don't always come from sort of average inputs essentially. But I think even off to studying statistics at a university level, that would be the kindest thing that I wouldn't immediately spot. And now having sort of spent a lot of time thinking about this and kind of building a product around this concept of probability. Anytime I hear the word average and alarm bell basically goes off in my head as to like, okay, what I like the sort of three or four different traps I can fall into when thinking about this problem through the lens of averages. Yeah, 

Speaker 3 (12:38):
I agree. I tend to think there are two big hurdles people have to overcome. The first is recognizing that you're in a probabilistic situation, which is almost all the time that you can't use a point estimate. You can't use an average, you need to understand the distributions and the samplings. And the second is what is the correct formula basically to use or how exactly do you mathematically navigate this probabilistic situation? And in my experience, most people miss the first step, they go to a point estimate and then it's already over before it started. You're not even wrong, right? You're in flatland, your answer has the wrong shape. And so I think there's a lot of value in having tools to help you navigate the mathematics once you're over the first step, but perhaps even more so tools, stories, experiences, histories that help people be more likely to raise the probabilistic flag, like warning we're entering probabilistic territory, that alarm bells should be going off almost all the time. And so I'm very interested in things that will help people get 

Speaker 1 (13:36):
More acclimated to that idea. Yeah, absolutely. I think one sort of common ish thing people do in spreadsheets is that know if you do want to understand the uncertainty of whatever you're trying to model, you know, some people might have three different scenarios, like a best case scenario and a worst case scenario and like a sort of average case scenario. Yeah. And then you'd kind of run your whole model for the best case in the worst case, an average case. And then you have these sort of three estimates for like, okay, this is what my outcomes could be. So some people do make an effort to do that in some settings and it's a step in the right direction, but actually under the hood, the maths doesn't really work out there. Right. You know, back to our buffet, we have these 10 dishes, which we can prepare in parallel so we can do them all at the same time if we said that, okay, on average, each dish takes one hour to prepare. 

Speaker 1 (14:24):
And in the worst case, it takes an hour and a half. And in the best case, it takes half an hour. If you were then trying to figure out what is the total time for the buffet, you might be able to get some kind of range based on sort of assuming they will hit the best case scenario. And that would be like the best case scenario for the buffet. And then assuming they will hit the worst case scenario. And that would be the worst case scenario for the buffet, but the math doesn't quite work out there. And it's mostly because our definition for best case and worst case changes from the start to the finish. So by best case scenario for a single dish in our heads, we probably don't mean the absolute best case scenario. We probably mean that like this is a 95% of the time. 

Speaker 1 (15:06):
It will be so in this quick and this, or whatever, and same for the worst case, the worst case scenario is just get ready for three years or something. Right. And so you don't actually think about the best case in the worst case. You all thinking about this sort of posable range, but the issue is when you start to think about this posable range and you're doing this lots of times, so we're doing this 10 times in this case, have 10 dishes. The equivalent plausible range for the total buffet is not when every dish hits the bottom of the range or every dish has the top of the range because every dish hitting the top of the range or the bottom of the range, it's actually extremely unlikely. It's like very implausible. No, you just described as how engineers estimate their time in a sprint, which is that every single thing they're going to implement is going to be the best possible scenario. 

Speaker 1 (15:54):
Yeah, obviously. Yeah. I think this is why it's so hard to plan projects because if you just do it on the basis of averages, then you know, there's a decent chance and needs to, one of your tasks is not going to be delivered on time. And if you do want to get some kind of balance on like best case, worst case scenario, if you have like 10 tasks or whatever you call it, actually just take the best case for each. And some of them will take the worst case for each of sum them up. And so they're new sort of rigorous way to do this is by running lots and lots of simulations for possible scenarios that could happen. And so, you know, it wants a simulation of the buffet. You know, three dishes might take less than an hour and seven dishes might take more than an hour or something. 

Speaker 1 (16:29):
And then all the simulation, they could all take less than an hour and so on. And if you ran a few thousand simulations, you could get an idea of like, you know, 95% of the time, how long does it buffet take and actually running these simulations is actually the only general and rigorous way to understand the range of possible outcomes for your buffet. Does that kind of make sense? And what you're talking about here is a Monte Carlo simulations, is that right? Yeah. Yeah. So in maths, this to be called a multicolor simulation, and actually you can have thousands of Monte Carlo simulations for a basic calculation that you might be doing. It's usually pretty cricky. The only way to really do it is to write a script that can loop through some calculation 10,000 times, and then show you, you know, 95% of the time your buffet takes between this time and this time. 

Speaker 1 (17:18):
And a big part of what we're trying to do with causal is sort of abstracts away all of this stuff around simulation probability distributions and let people just say, Hey, you know, each of my dishes takes between 45 and 90 minutes to cook. And now can you just tell me, like, what is the equivalent rage or the total buffet, how simulation does cover it, but there is something fun about the Monte Carlo name a little bit. And when I first learned about that, I don't unlike, I think both of you, I don't have kind of solid educational background in mathematics, but I later learned about it when I was kind of digging into the data science world of things, particularly with working with the R programming language. And they had essentially some exercises that involved doing these simulations, some very visual ones that I quite liked where essentially allowed you, they said, okay, you can calculate the area of a circle with the formula, or you can run a simulation where you essentially draw a circle on the wall and then throw darts that Landon random X, Y locations. 

Speaker 1 (18:21):
And if you do that a thousand times and count how many darts are on the inside of the circle and how many on the outside of the circle you can close in on the value of PI essentially, which I found somehow very amusing and fun way of going about things. I love that example. Yeah. I think simulation is a surprisingly powerful tool where if you can reframe any problem as almost like a probability question where you can run simulations, it's surprisingly generally applicable. And so in the example, you gave us there reframing the question of the area of a circle in terms of the probability of a dots landing in the circle versus outside the circle. And as soon as you reframe it, it has properties. Then you can just like run a bunch of simulations and it takes a while, but you'd have to be particularly smiles about it. I think most complex problems in maths they're often intractable. You know, it's very hard to express them as a clean equation that you have to solve. And even if you can't express it as a clean equation, there's often no general way to solve this equation as a reframing things in terms of like, how can we just do this really dumb thing a million times to get like a really good approximation for the answer is surprisingly generally applicable. Yeah. Very 

Speaker 3 (19:29):
Powerful technique. And especially useful for situations where you have multiple steps or branches, even just a few of those, they can be very simple to describe in human terms, if this, then that some chance and so forth. But once you have any complexity in situation, it often becomes impossible to get a so-called close form solution, which is what you were alluding to, where you have basically some formula you can write down, you plug in numbers and you get the result. Mathematicians always like such close forum solutions to the point where I think initially they kind of poo-pooed the Monte Carlo world, but I think now it's shown its power and folks are more open to the numerical approaches. The study of probability is so interesting because it pops up in so many domains once, you know, to be looking for probabilistic situations, you see them everywhere. 

Speaker 3 (20:13):
I can give two examples from my experience. The first was in college, I worked on this thing called RoboCup RoboCup is where you have toy robotic dogs play soccer. And these are dogs that can do basic seeing. And then you use video processing algorithms to extract information. Then you program the dogs to play soccer autonomously on this sort of toy soccer field. And anyways, one of the big advantages that our team had was the ability for the dogs to locate themselves on the field, which is, as you can imagine, this is sort of fundamental thing for programming dogs to play soccer. And the reason what this was so hard was because easy, like really bad cameras basically. So you're getting really choppy visual information. Really the only way to deal with that is probabilistically because the data that's coming in is so noisy. You can't do anything on it. 

Speaker 3 (20:59):
If this, then that basis, you basically have to say, okay, given all of these observations, I'm making about the different landmarks I know about on the field, what is probabilistically the most likely location for me to be in? And furthermore, what is my sort of probability cloud of where I plausibly am on the field. And if I have enough certainty by this probability cloud, then I can only take certain actions, like kick the ball towards the goal and so on. And then to give a very different example in the world of engineering management. I think it's very fundamental to understand the engineering is a risky endeavor, especially when you're like developing new products. This is an area where I think a lot of people think to deterministically. So one example that I like to give is imagine you have a multi-step software development process. You need to do a and B and C. 

Speaker 3 (21:43):
And this is actually kind of similar to the buffet example. Each one takes an engineer. One unit of work, an engineer can do one unit of work at any given time. Now you might think you should just assign one engineer, a one engineer B and one engineer C and in a totally deterministic world that works perfectly the gears. They all mesh, everything turns in a sentence. Perfect, but you have to recognize that there's inherent variability and how long these tasks take. And so what can happen is if you're running the entire team at so-called maximum capacity, then if anyone experiences a task that's slightly harder than you anticipated. You basically grind the gears for the entire thing, because a is holding a B is holding up C and then you go from this world of everyone is fully optimally. Working to everyone is basically stuck waiting for someone else and everything is kind of ground up. And that's where this idea of slack comes from, where if you're in a situation where you have uncertainty about how long things are going to take, and you have dependencies counterintuitively, the correct thing to do is to spend some of your time twiddling your thumbs, basically, because if you try to be doing stuff all the time, you're inevitably going to be getting in the situation where you're grinding 

Speaker 1 (22:48):
The gears up. And I think there by slack, you're referring to the concept of slack, not the product, right? And perhaps there was a book that was influential to me recommended by one of our mutual colleagues at Roku. That's essentially a management book that's titled slack and makes that very argument. It's sort of the queuing theory thing a little bit. And there's some things about creativity as well, but ultimately even if you just want to think of everyone on the team as being a worker automaton that needs to provide and units of productivity, it actually turns out you have a more efficient system when there's space in the system when there's slack in the system. 

Speaker 3 (23:25):
Yeah. And along these lines for people who enjoy thinking in probabilistic terms, I would also highly recommend principles of product development flow. This is basically a mathematical cue theoretic treatment of product development. And when I first heard that, I'm like, how can you possibly write interesting equations about product development? But if you just approach it with this of probability or alternatively risk, all kinds of interesting things fall out. So for folks who have a mathematical inclination, I suggest that book. 

Speaker 1 (23:54):
Mm. I guess our risk and probability the same thing in what we're talking about here, it seems like one is sort of just like the inverse of the other, at least in my kind of lay person's understanding. But I don't know if that's correct. Yeah. That's 

Speaker 3 (24:07):
My intuition. So you could think of risk in engineering delivery time means that there's a probability distribution. And in fact, it's probably long tailed where there is some chance it goes on time. There's a frankly small chance. It happens before you expect it to happen. And then there's the real possibility. It takes twice, three times as long and never gets done. Right. That's what I mean by risk. And similarly there's probability distribution around how customers are likely to value or not a given feature. And that's another thing that's important to consider. So you can't say customers are definitely, I like that. And in fact, there's some chance they like it. Sometimes they don't like it. Sometimes they really like it. And in the same way that you need to correctly consider distributions when you're planning your buffet preparation, you need to consider these distributions when you're doing 

Speaker 1 (24:48):
Product development. Yeah. I think just to add to that, when I think about sort of risk and probability and kind of how these concepts related, I think risk also kind of captures, I guess, kind of the magnitude of what could result from something. So for example, if you knew that there was a 1% chance that you die by driving a car, and that would be a much higher risk than if there was a 20% chance of getting wet from walking outside, starting risk also captures the actual impacts of some local busy event, right? Yeah. There's some good discussion of this, the 80,000 hours group, which I follow, they spend a lot of time talking about these kind of tail risk events, pandemics, which they were big on before we had one that captured the Western consciousness, but also things like meteor strikes and other events that obviously things that are climate-related. And in many cases, it is an acknowledgement of, yeah, the chance of this happening, the probability of this happening is small, but maybe this is sort of the expected value of something is the likelihood of it happening times the result. And so the result is this huge, huge event, like a species ending extinction event, even a very small chance of it as something maybe it's worth investing some resources protecting against, yeah. This 

Speaker 3 (26:04):
Is an area where even if you do take that first jump of thinking probabilistically, you can still fall short in particular, if the cases that end up mattering in the expected value calculation are outside of the intuitive probable range. So you can think of things like meteor strikes and nuclear war and so on. But one that's very familiar to us. Adam is earthquakes in California. So the chance of a very serious earthquake in California is on the order of whenever 100 years. So if you just take that, you know, it's basically outside the 95% confidence interval. So we can say if we weren't being too careful that basically we're not gonna have in our quake, don't worry about it. But in fact, the expected damage from such an earthquake is enormous. So therefore, any year the EBV on earth based in California is actually non-trivial. And therefore you should do some amount of preparation 

Speaker 1 (26:49):
That also highlights another challenge or fallacy, or just a way that this whole thing is non-intuitive for the way the humans think, which is you often hear folks in California speaking in terms of quote unquote, we're due for a big one, because you hear that, we talk about it that way we should expect one every hundred years and that actually masks or does not correctly capture the probability that we're trying to express. And so people convert that to more of a cyclical time thing like that. We expect the sun to rise once a day. In fact, that is not in a hold. It is so working on cultural and working with your users and customers who of course are, again, smart people, educated need to think in terms of probabilities for risks, for their work. And yet maybe don't have the same mathematics background that both of you have. 

Speaker 1 (27:39):
I mean, there's countless, I don't know, well-known fallacies, I don't know, expecting a string of coin flips to have fewer long runs of heads and tails, for example, than it does in actuality. But what are some of the things where either one, you see folks have their intuition not matching what reality is, and then two, what are some things you found in the product, or maybe it's even more of a, almost like a marketing thing and explaining thing to help folks bridge that gap without necessarily getting the mathematics degree. Yeah, for sure. Yeah. I think we've had a ton of learnings on the more sort of marketing and positioning side of this kind of product in the very early days. You know, our mission was to really focus on this problem, but as he stopped. And so when we, you know, on our landing page, we would literally describe causal as a probabilistic modeling tool. 

Speaker 1 (28:26):
That means something to us. But I think what we didn't realize is that for people, without a maths background words like probabilistic and words like Monte Carlo simulation, they're just quite scary. I, I initially found this a little bit frustrating because you know, the term probabilistic model to me, it means like a very specific thing. And it was pretty hard to try and describe this concept to folks with less mathematical backgrounds, but actually I can really empathize with it because even in my own sort of math degree, I really struggled with terminology and notation and things like that. And I think a big problem in kind of maths education generally, is that there's a lot of focus on notation and terminology and you kind of miss the forest for the trees. And so, you know, even when I was in my second or third year of university, anytime I would see a capital Sigma, you know, the big sort of some symbol, which is basically every way in every branch of maths, you're going to be summing things up and he's talking, I'd see like the, some of like some expression I'd immediately think, oh man, this is so hard. 

Speaker 1 (29:27):
This looks really complicated. There's all these symbols going on. And so I definitely felt that pain of being intimidated by terminology and notation. And I think that was part of the problem initially, when we were using words like probabilistic and we were using words like multicolor, you know, it took me sort of, yeah, I say in my fourth year of my math degree, I didn't have notation on IC anymore, but it took me a long time to get over that. And I think a lot of people who didn't like maths in school or feel like they were bad at maths, I think a lot of it just comes down to notation. You know, once you're introduced to algebra, you start seeing all these symbols like X and Y and so on, and it takes a while to get comfortable with that. And it's easy to fall into the trap of thinking, oh man, I find the notation confusing. 

Speaker 1 (30:09):
Therefore I am bad at math. Therefore I shouldn't say this thing, but I think getting pasta language, getting positive notation is actually a big huddle. And so for causal specifically, you know, it took us a few months to figure this out, but we stopped using words like probabilistic. We stop using words like multicolor. I think generally people understand the idea of uncertainty. And so in terms of how we position, I guess the probabilistic aspect of chordal is that we usually describe it in terms of, you know, Hey, if you're uncertain about a particular number and so writing a single number, you can say, Hey, I think there's between three and five, or I think it's between five and 10 and people get pretty intuitively understand ranges. They can probably come up with a range for any quantity and that day-to-day life that they might want to model and saying, like, I think something is between five and 10 doesn't require any sort of technical knowledge. 

Speaker 1 (30:56):
It's sort of pure intuition. And so Caldwell people just need to apply their intuition at the point where they can do it well. So at the point where they can estimate a range for a particular quantity where the intuition breaks down is, you know, you know, have this model with a bunch of formulas, a bunch of calculations where you're taking all of these five to tens and tens and twenties and so on, and combining them in some weird way to get a ton of results. That's where intuition really breaks down. It's actually very hard to punch those numbers in your head. And that's why Caldwell handles it for you. It runs, you know, 10,000 simulations and then just shows you the sort of 10 to 20 results rather than you having to worry about that side of things. So I think we had lots of learnings on the sort of positioning and kind of the marketing side of things in terms of actually getting people to think more probabilistically. 

Speaker 1 (31:41):
I think most of the folks that use causal previously used spreadsheets, and if you've had to build a financial model in the spreadsheets, you're probably somewhat familiar with the idea of best case and worst case scenarios. But I think most people just don't do that because it's just very fidgety and requires a bunch of formulas and things like that. And so actually getting people to start thinking in terms of ranges has been pretty easy because people have wanted to do that. Anyway. It's just so much of a pain to set that up in a spreadsheet that they haven't ended up doing it. And so being able to just rice in and expression like five to 10 in causal comes very naturally to people and they do tend to do that quite a lot because cool. How does the complexity of all of that in terms of the output they see, you mentioned just seeing you put in a range or a series of ranges and you get out a single range, but there's also, maybe you found ways to represent that visually in plots or yeah. 

Speaker 1 (32:29):
So representing it visually is tricky. I mean, it's a causal under the hood, you know, it's running all these simulations. And so it has a lot more information than just the range of your possible. It has these sort of precise distribution of your possible outcome. As you know, the range might be five to 10, but it might be more likely to be closer to 10 than closer to five. And so on where I might have the sort of bi-modal thing was really likely to be close to five or 10, but not likely to be anywhere in the middle. And so there's lots of different distribution shapes that might underlie a range like five to 10. We found that most folks don't have too much familiarity with reading probability distribution charts. It is a feature in coal. You can actually see it like a bell curve if it happens to be like that or other equivalent charts, most people aren't too familiar with those. 

Speaker 1 (33:14):
And so most people don't end up using them. What people are fairly familiar with is sort of like fan shots. So if you're projecting something over time, you know, you might have like a single line or something. And then instead of a single line, you might have like a sort of fanning out range where this kind of visible off of bounces range and visible about. And most people really choose. If you understand what a fan chart looks like I said, those are really common, but unfortunately it does kind of hide the underlying distribution and we haven't yet figured out, but really it's, here's a way to show people the actual distribution in a way that they'll understand. 

Speaker 3 (33:44):
I do feel like those fan charts, which now I know the name for that's useful are perhaps the closest thing we have to the line through a dots in terms of comprehensibility and universality. I've seen those a lot in the financial domain where you have a balance or a bankroll or similar investment balance and you run a hundred simulations and you can kind of get a sense of the probability distribution if you have the right amount of lines and your fan chart, because you see that there's kind of more lines in the middle and fewer lines and the scraggly edges are not perfect, but it's pretty intuitive. I also like those cause they do show a dynamism. So if you're looking at a bank role, for example, you see that some of these lines, they really dip close to zero and some go way out, but then come back down. And a lot of them just kind of chunk along. So you get some sense for the randomness. 

Speaker 1 (34:29):
Yeah. We've had to put a lot of thoughts into how much detail we want to show in these kinds of visualizations. So when it comes to fan Childs, for example, Kozol does have all 10,000 of this simulations, then we could fall on, you know, each of those 10,000, maybe with like a sort of 1% who passes you or something. And so then you can actually get an audio distribution, but it just adds a lot more complexity to the visualization. And so we've had to try and find the balance between sort of complexity and comprehensibility, where if we try and be super rigorous and show every single simulation on the charts, chances are most people will look at it and get a bit confused and not be able to make any sense of it. Whereas if we kind of show that sort of 90% rate on the 95% range, it's much more understandable and at least people will have an idea of a range of possible outcomes. And then maybe if they want, they can kind of double click and zoom into the distribution itself box. It is very challenging to actually visually represent on certainty. There's a few research departments that a few universities that are doing a lot of work into figuring out the best ways to visually represent uncertainty. But yeah, it's all about the balance between sort of complexity and core principles. Now we've 

Speaker 3 (35:36):
Talked mostly about in the sense of going forward. So you were about to begin preparation of the buffet. What should you expect in terms of the completion times approximately one hour from now? I don't think there's this very interesting world of probability, which is basically going backwards. It's, you've observed that everything completed in an hour and 15 minutes, what does that mean about the underlying tendency for us to complete individual sections of the buffet and there all kinds of other examples that we could talk about. I'm curious if you see those sorts of use cases and causal, or if you have other thoughts on that space, 

Speaker 1 (36:07):
We definitely see less of those use cases. The one time it does come up is if you have a bunch of historical data about a particular one to see maybe you have a bunch of historical exchange rates between the dollar and selling something. If you then want to kind of use that exchange rates to project something forwards, it is helpful to kind of look at what has been the distribution of this exchange rates historically. And then let's just assume it'll probably have a similar distribution going forwards. And so in that way, instead of just sort of plucking a range out of thin air of like, oh, I think the exchange rates between no 0.9, 9.99 or something like that, you kind of actually infa the distribution from historical data. And that is a feature that we do have. What, if you have a bunch of historical data for something we can sort of try and fit in empirical probability distribution is what it would technically cold onto that so that you don't have to put your finger in the air and come up with a range. We see a lot less of that. And the more useful thing does seem to be being able to apply ranges based on your own assumptions rather than figuring out ranges of distributions from historical data. 

Speaker 3 (37:09):
Yeah. Maybe we can just talk about some examples from our own experience of this type of probability. One example that I think is really cool and this one's due to Sammo Bertia, I hope I'm pronouncing his name correctly. This is Samuel with Bismarck analytics. We can link to him in the show notes, but he's made this point that with how we've historically thought about archeological discoveries, our timelines only go backwards. So say for example, we'd find the first cave painting and we date it to 5,000 years ago and we say cave painting has been around for 5,000 years. And then we find another cave painting and it's 8,000 years old. And then we say, I guess cave painting has been around for 8,000 years. Now the first observation is that if you take this naive approach, our timelines are only ever going to go backwards because any time we discover a newer one, okay, we've known about that. 

Speaker 3 (37:57):
Anytime we discover an older one, our timelines for when humans were doing certain things are going backwards. And perhaps then the correct way to think about this probabilistically would be to say that when we discover the 8,000 year old cave painting, there's some underlying distribution of cave paintings, some of which are probably older than 8,000 years old. So therefore the correct estimate is probably older than that. And if we were in fact doing that correctly, we wouldn't always be getting older. We would be kind of getting more and more refined around the true date on either side. It's just one example of how, if you don't think about things and careful probabilistic terms, especially when you're doing this sort of backwards projection onto the underlying distribution, it can very easily make mistakes. 

Speaker 1 (38:39):
Yeah, that's a really interesting example. I think this actually came up during some of the stuffs courses that I did at the university. We did a course on Bayesian inference. So using kind of Beijing theory equivalency, and I think this is one of the few areas where people have actually been applying sort of a Basian ideas of probability in practice in real life. And I think we actually had a bunch of examples in Allison of election, specifically around archeological digs. And if you dig up something that's 50 layers of, I don't know, sand deep or something. And you think that that's data from a certain period, how should you actually think about your new best estimate for how long we can do the cave paintings? And so there's a bunch of maths that can actually sort of help you with that. And from my understanding people all using that maths in offloading stuff. Nice. I'm curious as to how you guys personally think about how much to trust the numbers, how much to trust data is statistics. I found that for myself, I'm just very skeptical of any numbers that anyone tries to throw at me. And I'm usually much more convinced by a theory or an argument that I find highly plausible than by someone trying to convince me of something using data. Where do you guys pull that spectrum? And like in what contexts do you trust numbers that people throw at you? And in what context do you notice? 

Speaker 3 (39:56):
Oh man. So this gets us into the conversation of what you should believe when you read a newspaper, according to a study. And so for me, that's a very little, or basically nothing. And so I have a lot of trust in statistics and numbers and experiments, but you've got to consider the whole ecosystem. And when you're looking, for example, at the ecosystem of publicly described science, there are many, many steps where the data gets systematically corrupted. And so what you're likely to read at the end is just not that useful. So just to give some examples here, when a newspaper reports on a scientific study, they're very likely to report an incorrectly because of probabilistic literacy. And then even among the studies, they choose to report on that. They're sampling from the universe of studies and they might have biases or reasons to only report a subset of them. 

Speaker 3 (40:44):
And then furthermore, the stuff that gets published that is systematically corrupted, there's only certain types of results they get published. And then in terms of the data that goes into both the published and unpublished studies, there's a lot of fraud and other issues with it. And so by the time you get out to the end, it's just not that useful. And if you want to have a chance, you basically need to do a meta review or a meta-study. I forget what the exact term is, maybe no, but basically where you round up all of the studies that have ever existed, both published and unpublished and try to synthesize all the data to say something useful. So because these universities tend to be so complex because of all the principle agent problems involved, they tend not to trust them that much. But when I have my hand on a specific experiment that I understand end to end and ideally was pre-registered, then I'm quite likely to trust it. 

Speaker 1 (41:30):
And preregistered here means they didn't extract a meaning or find meaning post hoc once they looked at the data, but rather than they're using it to test or falsify or prove or falsify a particular hypothesis, 

Speaker 3 (41:43):
Right? So this is one of the areas where historically scientific publishing has gone very wrong. Let's say you have a new drug, for example, or you have a hundred new drugs and you privately conducted tests on one hundreds of the drugs using the standard 95% confidence interval. Well, you would expect that five of those will falsely return. Even if all the drugs are platelets, they do nothing. You would expect that five of those placebos return given your 95% confidence interval is by definition that they are helpful. And so if you have the opportunity to publish or not publish whatever studies you want, you can just publish those five and say, Hey, look, we're all, we have five drugs that are magic. And in fact, you're attempting to fool the public by randomness. Whereas if you pre-register all 100 studies, then you can't do that. People can see like, you know, wait 95% of the stuff that you think might be useful is actually not useful. So therefore you're just not a very good developing company 

Speaker 1 (42:34):
That makes me think of a related concept in terms of like, yeah, 95% sort of effectiveness, which is essentially medical tests. And that there's a pretty strong argument against sort of testing that you would think that the best thing to do, whether you're talking about a disease or early cancer screening or anything like that is just test as much and as often as possible. But the challenge with a lot of these things is that you get this asymmetry between the false positives and the false negatives, which essentially if the test is even 99% accurate, but the disease only appears in one out of every 50,000 people, the number of people who get the false positive that is say saying that they have the disease when they don't vastly outweighs the people that actually get correct positives on it. And then they spend a bunch of time with stressed out people thinking they have a terrible disease. 

Speaker 1 (43:26):
And in fact, they want the doctors to make the judgment call of there's some reason, some symptom we see here that makes us want to do the test rather than kind of a proactive test, which I thought was very interesting. And again, to me was a surprising result. I think coming back to that intuitively you don't think of a test that has, for example, 99% effectiveness as being something that would produce such kind of skewed the wrong, or it would just misleading results. But without knowing that other number, which what's the incidence of this particular disease in the population that you're running the test against, you actually don't know what the balance of false positives to true positives is for me, the question of whether I'm convinced by data. Certainly I think for me, it does come down to putting numbers on things. Quantifying things brings a, I don't know if rigor is quite the right word, but perhaps a concreteness. 

Speaker 1 (44:22):
When you say something is really, really big versus saying it is 50 meters tall, those two have very different qualities to it. And I feel when people either do bring numbers in either from their own volition or because they're forced to by scientific practices or something like that, that, that actually sharpens the thinking. Now that doesn't mean that numbers are a magic wand by quantifying things and turning that into data sets, whether it's a spreadsheet or something else or the new favorite magic wand, which is data science. That just because you bring those things in and now your results are unimpeachable. But rather that I, in general, that is going to probably do better than more kind of broad abstract kind of reasoning by analogy or something like that. But yeah, I guess certainly the specific case marked names of studies as reported on in the news is something to be very suspect of, but looking at a data set and using that to draw some conclusions, I think can be a very powerful way to understand the world tomorrow. 

Speaker 1 (45:25):
I might also turn the question back to you on product development and say to what degree being a, certainly a very numerically literate person, to what degree do you use some kind of data or quantification in making product decisions or business decisions, or do you really guide that? Especially maybe in the early days were just the end in terms of number of users, number of customers, total time elapsed, just isn't big enough. We need to just kind of go with building what's in your heart, as I said earlier, or following your product intuition and not getting hung up on trying to make sense out of a small dataset. Yeah. So I think ironically, we very much are on the side of our intuition and conviction on things. I think personally, when it comes to big product things like you're just not going to find the answers in any data set. 

Speaker 1 (46:14):
And so one's a really big thing that we kind of grappled with from day one was we're building this tool for working with numbers. It's very general and so on. What should the UI for this thing actually be? And, you know, we were always kind of aware that, well, maybe we could make it look a bit like a spreadsheet because that'll be more familiar to people and so on, but you know, maybe we want to move people away from that and get them to stop thinking in those terms of moving on to don't do that. We actually kind of had our own proprietary UI until about four months ago. And maybe about six months ago, we decided actually, you know what, a lot of people are having trouble getting onboarded, you know, no one is explicitly telling us that look, give me a spreadsheet in space. 

Speaker 1 (46:51):
No one was explicitly telling us that. But you know, there was a lot of like friction. There were a lot of things which just quite one working out. And so we had to, you know, in the absence of data, we had to ourselves come up with an analytical model of the world. Like, Hey, we're having these problems because our interface is too hard to use for new people. It's too confusing. And so we should build the spreadsheet. And I can't imagine, you know, maybe we could have run some survey asking people like, Hey, you know, would you prefer a spreadsheet interface or another? So, you know, but again, I think like designing a survey in a way that I would actually trust it can be really tricky. And I going to how much I would trust the results of that kind of survey. So I think product stuff generally does not come from any kind of data, it's more around our own intuitions. 

Speaker 1 (47:35):
I'm very happy to trust data, to sort of, to an existing thing that we've created. I think data is very good for tuning something that you've come up with. So I think like an onboarding flow is an example of this. So, you know, yeah. I think you guys had like a previous episode just about onboarding or something like that in the early days on cost. Onboarding is a big challenge for causal and we have like a guided onboarding with most, you make an account, we then showed like little dots on different parts of the UI saying help click here now, type this thing in now, press enter and so on. And to guide people through kind of the main flows of the products. And that's the kind of thing where we can come up with the structure of like, okay, we think these are the five steps and this is what we should tell people. 

Speaker 1 (48:15):
And then we can look at the data to kind of optimize this structure that we've come up with. And so we can see that, okay, you know, loads of people are falling off after step three. This is probably a problem that we should probably change that data. Wouldn't tell us what the steps should be. We have to come up with that structure ourselves. And then once you have the structure, then data is good for refining it and tuning it as a that's really how I see data as like, you know, it's up to us and our own conviction to build a main structure. And then we can use data to kind of refine it a little bit. I don't remind me, we have something, a product manager from Pinterest told me that they did their, at least at the time, which was to use split tests automatically just to check there, isn't a regression in whatever their core metrics are, which might include, you know, monetary things, people converting to purchase or whatever was there, but also things having to do with, yeah. Basically check their core metrics and make sure this exciting new feature they rolled out. Didn't just cause something important to tank. It's kind of a safety check. So it was almost more of a regression test rather than something that was intended to decide product direction. This 

Speaker 3 (49:20):
Conversation reminds me of a couple of things. One is so-called AA tests where you test your AB testing framework and analysis by making the two sides of the test. Exactly the same. And so that's a good way to see if you are likely to fool yourself by randomness, because if you come back with a result that a is bigger than a, well, something's probably wrong with your probabilistic reasoning. When you mentioned the idea of a spreadsheet or a phrase, that's something you see in a lot of tools for thought and productivity apps. For example, notion an air table will have this idea of a sort of spreadsheet. Like thing that you can put in. It reminds me of the phenomenon of Carson is Asian, which is the tendency of crustaceans to evolve into crap. Like things spread. She doesn't have the crab of the productivity tool world. It's like everyone kind of wants to be a crab slash spreadsheet depending on where you are. 

Speaker 1 (50:08):
Yeah. That's pretty funny. We had similar things of, one of us investors slash advisors is a chap. Who's kind of been in the financial modeling game for a very long time. He sort of has a business selling Excel, financial model templates. And so he's tried every sort of number crunching tool under the sun rating. And early on, you know, he basically predicted this and he, he of told us that like every tool that I've ever seen, it'd be crazy for this eventually ends up looking like a spreadsheet. That's what I'm saying, you know, do whatever you want with that information. But he called it about a year and a half ago. Yeah. Well sometimes the process of being a product creator, especially when you're trying to do something truly novel is to try all your weird and exciting ideas. And unfortunately most of them will probably turn out to be not effective. And then you realize why it is that the boring standard thing that everyone uses is boring and standard is because of really works. But hopefully you find those few weird ideas that in fact are breakthrough and can make a difference in the world. Yeah. We almost sort of had to figure out from first principles that a 2d grid is a good way of displaying it. Two dimensional. 

Speaker 1 (51:20):
I like a little bit that approach of throwing out assumptions and throwing out kind of a sense of, well, we're doing it this way because that's the way we've always done it. Do I think it's a very easy to build products that way to say, okay, well obviously you start with the login page because everyone has a login page and then you have a page that's like this and a screen that's like this. And you're just going based on assumptions of following established patterns and throwing those out and saying, okay, now what are we trying to accomplish here? And what if we designed something truly new and more often than not, you do end up back at those established patterns because they're good for a reason or they work well for a reason, but I feel like it's a truer and more pure way to arrive at those kind of building it up yourself as opposed to kind of just imitating without knowing the underlying reasons. Yeah. This 

Speaker 3 (52:07):
Is actually reminding me of the importance to my mind of studying combinant torics and probability as a predecessor to statistics. So a lot of folks, I see these days, they study physics and so they just get the formulas for like, how do you do a two-tailed T test or whatever? And I don't have the underlying intuition, whereas I think it's much more useful to have the underlying intuition of especially coming into RX, which is the study of counting. And therefore it gives you probability. So yeah, it folks are interested in this space. I would suggest starting with how to count things and coming into 

Speaker 1 (52:38):
Work well, let's wrap it there. Thanks everyone for listening 

Speaker 2 (52:43):
Feedback. Write us on Twitter at Misa HQ, by email, hello and misa.com and help us out by leaving a review on apple podcasts and Tamar. I'm glad you're building a tool for thinking in probabilities because I think we all need it. It's been a lot of fun. [inaudible].

