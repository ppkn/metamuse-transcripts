Speaker 1 (00:00):
Also something that makes it very unique as this, like you're, you're basically floating through space and you're zooming deeper into your hierarchy. And all of this is like a perfect illusion of seamlessness when it's actually not seamless at all. 

Speaker 2 (00:22):
Hello and welcome to Metamucil. Use a software for your iPad that helps you with ideation and problem solving this podcast. Isn't about music product. It's about music company and small team behind it. My name is Adam Wiggins. I'm here today with my colleague mark McGranahan Hey Adam, and my colleague, Julia robots. Hi Adam and Juliette. You have now made to have it two years in a row to spend the entire winter in a sunny location away from your home in Germany. How's that working out for you? You can repeat that again next year. 

Speaker 1 (00:56):
Oh yeah. I mean, I guess we'll see about next year at what traveling is going to be like in the future. Um, but at least for the past few years, I've really enjoyed that. I think, I mean, I love my hometown Berlin, um, and I love being here in the summer, but in the winter it can get quite gloomy and dark and cold and I'm very much a sun person. So, um, yeah, I've really been making good use of this remote company set up and, you know, make your own work hours for the most part. So spending lots of time in adventurous places, kind of splitting my work days in half, which is something that I really like to do get some work done in the morning and then do something nice outdoors and then work some more hours in the night. Um, it's been really been a really nice balance for me throughout the wintertime. 

Speaker 2 (01:42):
You have a very impressive ability to get stuff done while also interleaving it with adventure, you'll, you'll ship some major new feature and then go whale watching and then fix a bunch of bugs and then go kayaking. 

Speaker 1 (01:58):
I'd be like, guys, I'm going to be 20 minutes late from the meeting. Uh, I've just got getting back from a school, but 

Speaker 2 (02:04):
That's yeah, absolutely. But it's also a reflection of the, kind of the work environment we built. Mark. And I talked about this on previous episode of trying to make a space that is flexible for all of the people on the team to live the kind of life they want to live. And for you apparently scuba diving and, uh, whale-watching and kayaking is, is the life you want 

Speaker 1 (02:23):
To live. Yeah, it's definitely been amazing to not have to separate your life so much between like work and traveling, like usually traveling from you always happened on vacation. Um, and I actually find the mindset that I'm in when I travel, when I'm in a different country to be extremely stimulating in many ways and actually that to make me more productive. So being able to mix that has been quite the blessing. 

Speaker 2 (02:47):
So our topic today is iOS development. And then from you specifically kind of our gesture recognizer system and why that's so challenging to implement. But I thought maybe for context, for people that don't know how I asked development works either because they know about software development generally, but not necessarily kind of mobile development or even people who aren't necessarily that familiar with how software gets built, they might like to know what does it look like for you? You sit down in the morning or maybe the afternoon to work on some features or fix some bugs. You're going to start crafting a muse out of artisinal ones and zeros. What does that actually physically look like? What devices are you using? What software are you using? 

Speaker 1 (03:31):
Yeah. So in terms of devices, I use a Mac book first and foremost, as far as I know you still can't develop iOS or Mac software on any other platform. So that's where everything starts. And it comes with, uh, with the IDE basically to develop for the iPad or iPhone, which is called excavator 

Speaker 2 (03:51):
ID being integrated development environment. 

Speaker 1 (03:54):
Yes. Correct. So basically it's kind of the entire toolkit that you need to write software for the iPad or the iPhone. You write all your code there, you compile it there, you debug it there. So what I usually do, um, is that I plug in the actual physical iPad. The XCOR also comes with a simulator and you can run all of your M I O S apps in the simulator itself. So basically just brings up a little screen on your computer that looks like an iPad or an iPhone, and it can do most things there, but for an app like ours, which is extremely gesture driven, and we use the pencil for many things, it's a bit tedious to actually, um, work with the simulator and some things aren't possible at all. So I work with the physical device plugged in. You can actually also build to it wirelessly as of a couple of years ago, but it is a little bit unstable. So I try to just depend on the cable there. Um, and yeah, then I just write some code that like click one button and then it runs on the device and then I can test everything there. 

Speaker 2 (04:59):
And this is the swift programming language. Um, we're storing our data or sort of the persistence layer is core core data. Do we use any other fancy libraries or APIs or is it mostly just kind of the apple gives you a pretty complete kit for development, everything from the editor through to the language and all that stuff, the simulator, like you said, uh, whereas like I come from mark and I actually both come from more of a web development background there. You're putting together more mix and match, uh, the tools, the language and the different pieces, but here you get this one kind of it's the apple style thing. You get this one pretty complete kit. 

Speaker 1 (05:34):
Yeah, yeah, pretty much. Um, so I think it's fairly rare for, um, an IRS project to have no like zero dependencies to any sort of third-party libraries, but ours are actually quite minimal. I think we have something in there, for example, for like zipping and unzipping files. That's something that, as far as I know, it's not built into the, um, IRS kind of standard library, but for the most part, really like the iOS SDK is extremely comprehensible. You can do all kinds of things with a day over the years, they've added, um, much more stuff, especially from kind of open source, third party frameworks that were very successful have often been integrated in one way or another into the, um, IRS ecosystem or they've basically rolled to their own, their own version of it. So our dependencies on external frameworks is actually quite small. And at 

Speaker 2 (06:28):
One point we were doing the, maybe this is back when I used was still a lab project, but our persistence layer was Firebase, which this kind of mobile backend data service from Google. Um, what was our, I think w we liked that pretty well developer experience wise, but what, what led to us kind of replacing that with the apple standard on device storage? 

Speaker 1 (06:49):
Well, I think the main motivation here was that we basically didn't want to be dependent on Google and kind of giving, giving our users data, um, to be stored on Google servers. So I think that was, that was the main motivation. Yeah. 

Speaker 3 (07:02):
Yes. Speaking of sending or not sending user data to Google, I'm really proud that we don't have any third-party analytics libraries integrate into muse because these are notorious for scraping all kinds of data and sending it to a bunch of third parties. You saw this recently with zoom, for example, where they had, I think it was the Facebook SDK integrated, and apparently unbeknownst to them, it was sending all kinds of user data to Facebook, presumably for advertising purposes. Um, so I think that's a really healthy thing that we have with our current minimal dependencies. 

Speaker 2 (07:31):
We do have analytics, but this is, uh, a system built by you or, or it's sort of a roll our own type thing. Yeah. And 

Speaker 3 (07:38):
It's, it's extremely minimal and deliberate. So every single field, which is like basically like three or four that we send to this analytics service are handpicked by us. It's in our code. It's, it's explicit versus a dependency that's updating every week. And it's scraping new random things from the us and sending it to third-party servers where we have no control over it. 

Speaker 2 (07:58):
Mark, you end up building the backend side of things, Yulia, you do the client side of things. How do you coordinate around that API? How do you, how do you figure out how to make those two ends meet? 

Speaker 1 (08:09):
I think for the most part it's been pretty lightweight. We chat on slack about what's needed for a certain thing. Um, often mark ends up kind of drafting a, a notion document or something. 

Speaker 2 (08:20):
So like API docs or a design specification kind of thing. Yeah, exactly. 

Speaker 3 (08:25):
So, so typically these notion docs will have first the mental model, which I think is really important. Like what's the shape of the domain here, why that key objects and key verbs, and then like a sketch of the HTP API, which again is usually very simple and then a discussion of the behaviors that are behind. And 

Speaker 1 (08:41):
Then as soon as we get into implementing that, um, it's usually we end up being online around the same time and I'm telling him, okay, I've just implemented this API. Uh, is it the plight yet? Can I, can I start hitting it? And then I've just, you know, depending on what it is, I sent some sort of event and Mac checks in the logs. If he see if he's seeing the right thing and, you know, often there's a few things from there that we need to fix. Like something is not encoded in the right way, but we basically just tackle that together via slack or a video call. And 

Speaker 2 (09:12):
Maybe just to round out the tech stack discussions, since he referred to the front end there with, you know, swift and core data on the backend, we're basically doing Ruby Postgres and Heroku, which for mark and I is kind of our, our very standard toolkit. I think they say, you know, we came out of this research lab where our goal was to push the boundaries of technology and what, what we can do there and try lots of weird and interesting cutting edge things. But once you have, once you're moving into the realm of production and commercial products, they say, choose boring technology, choose the boring things that are workhorses that have worked really well. I've used Postgres, for example, for, I don't know, now 15 or 20 years. Um, and there's always a shiny new thing, but the stuff that's really reliably and the stuff that has performed reliably for you for a long time is often just the thing. 

Speaker 3 (10:02):
Yeah. I'm really happy with our backend stack and of course, Roku, but also Postgres in particular, such a great database, super rock solid, super flexible. And now we can use it for both our sort of online data, as well as our analytics data. 

Speaker 2 (10:16):
Yeah. And a quick shout out on that kind of from the product perspective to data clips, which is a subtle way to bundle up a SQL query in a form that you can share it as a, um, as a webpage, we use that quite a bit as our kind of our ad hoc analytics sharing system. All right. Well, let's get into the meatier part that gave that hopefully that gives some good context for, um, technical or, um, less technical folks about exactly what the pieces are here. Now getting into something that is pretty, and all of that I think is fairly sort of standard stuff that you might see in a, in an iOS app or an iOS app that has a small backend, but getting into muse, which is trying to really push the boundaries on what you can do with the tablet app, with these unique gestures, the different treating the, treating the pencil differently from the hands that there's multi handed gestures and all this. So we have quite a bit of both design and engineering effort that has gone into our, our gesture system. But maybe we can start at the very beginning Yulia, what is adjuster 

Speaker 1 (11:19):
It? Gesture is a, it's a good question. Actually. I don't think I've ever defined that for someone, um, in terms of iOS development, there's actually, um, a whole system around gestures and gestures can be, uh, of one or more categories. So there is a pen gesture, which would be just setting your finger down on a screen and moving it somewhere. You might be actually touching an item that you want to drag along, but you can also, you know, pan for any other reason, for example, to draw something, then there are things like swipe gestures, which are also a pen in a way, but they're like distinctively, like flipping through pages, then they're scrolling, which is a more of a continuous leaving your finger and scrolling something. That's a scroll gesture, um, there's pinching, which is sort of you're zooming in and out of thing of things. And there's a whole bunch of, uh, other gestures that you can, you can combine in your app to achieve different things, but they're usually triggered with your finger or in our case, or in some other apps cases also with a pencil. Yeah. 

Speaker 2 (12:22):
And probably from a user perspective, you don't even think that much about something like tap a double tap, a swipe, pinch these all part of the magic and the beauty I think of multi-touch screens and why they've, um, sort of taken over the world in terms of interfaces is that they do seem so natural. And it seems so obvious the difference between for example, a swipe, a scroll and a pinch. But in fact, it's quite a bit of logic to, um, make sense of that stuff. And I have experience with sort of mouse, um, wouldn't call them gestures, but basically interpreting what the user does on a desktop computer with a mouse, um, in my past life as a game developer and there things are actually a lot simple because a lot simpler because you generally have the X and Y position of the cursor and whether the buttons are down and there is a time element for some things like double clicking, but it's pretty minor. 

Speaker 2 (13:17):
Most things are really discreet. Uh, the thing that I think really opened my eyes on this was, um, we both were at UI con last year where you gave a talk and another talk. There was, uh, Shannon Hughes who worked for Omni group. They make the some great productivity tools like OmniGraffle and OmniFocus. And she had worked on, I think the iPad app for one of the use and had done gone pretty far on these, um, these gestures and even has written a, an open source library for basically making a diagram. And she showed this, these kind of these gesture disambiguation diagrams, uh, in real time. And you could see that actually the there's this huge time component where what makes a gesture, a gesture is not a discrete moment in time. It's a collection of positions and it touches in different places and movements of those touches over time and the accumulation of those things eventually resolves itself into the system deciding, okay, I just saw 

Speaker 1 (14:16):
A pinch. Yeah, exactly. And gladly we're getting, um, pretty much all of that for free from the iOS SDK. So you could, if you wanted to, and you, you know, you had the time, or it was just an interesting experiment for you. You could actually write all that yourself. So you can get just very raw touch input events from the system. If you have a screen, you can basically just implement a couple methods that will fire whenever a finger goes down and move somewhere just with a position and nothing else. And you could go from there and build your own, you know, this now I think these fingers moved from each other, so it must be a pinch out, but I'm gladly. The folks at apple have gone through all of that work for us and, uh, develop this concept of a gesture recognizer that you can just attach to any view. And that will make that view, respond to specific gestures, for example, a pinch, and just notify you when, when that gesture first starts. And then when it changes and also give you for a pinch, for example, it'll give you the scale. So it starts out with a low scale. And then as you pin, as you move your fingers further apart, the scale value will change and it will just notify your callbacks and then you can zoom or do whatever, whatever else you want to do with that pinch. 

Speaker 2 (15:33):
Now, if I was to look at the raw data, and I think I've seen test programs that do this, the screen or the system that's reading these touches, of course doesn't know which finger I'm setting down. So the difference between, you know, for me, where I can see my hand, it's pretty obvious that if I, if I put down, for example, my thumb and my index finger near each other and move out, you know, that looks like a pinch gesture or put them down further apart and move in. That looks like a pinch. But the difference between doing that with my thumb and my, uh, pointer finger versus doing it with each thumb on each hand, which you could totally do, but the system can't tell any difference. It just to text touches in certain locations. And then those touches start moving. 

Speaker 1 (16:17):
Yeah, exactly. And that's actually what makes everything so complicated that we're trying to do. In fact, the pinch is even recognized when fingers only move by only a very few pixels. So one example that I can give from, from our app where this was a bit of a puzzle that we had to solve is we want to allow two fingers crawling on a board. That means you set down two fingers and you move them in, you know, either to the left or right to scrawl the board. But we also have this sort of global pinch gesture, recognizer that listens to you pinching out to zoom out back to the parent board. And that gesture is triggered by at least in the past has been triggered by even the most minimal movements. So we wanted to build the app in a way that is that it's super fluid so that it responds to your touches right away. 

Speaker 1 (17:12):
That means that even if you set two fingers down on the screen and they converge by maybe five pixels towards each other, the system will consider that a pinch and we'll immediately start the zooming transition. So when you actually just using two fingers to try and to scroll, there's basically no way that you can, you know, you're not a robot, you're not going to be able to keep them completely parallel to each other. So we had to add a bit of custom, this ambiguation logic, where pinch is only triggered after the fingers moved. You know, maybe by a scale of 1.1. Um, so by, you know, more than 10 or 20 pixels, depending on where you started with your fingers, and that adds a little bit of delay to the system, you know, actually responding to your actions when you do want to pinch, which is a trade-off obviously, but it's basically the only way that you can make these two gestures work together. Um, and descending, UBIT disambiguate them in some way. 

Speaker 3 (18:13):
Yeah. This delay issue is really interesting. One of our top level design goals from you is, is that it's super fast and responsive. So the idea is as soon as you touch the screen and do something, the app should respond so that you always feel like you're directly manipulating your content. And as Julie was saying, that's really hard with these gestures that are potentially ambiguous. And in some cases we've taken this approach where you try just to have a very small delay, basically in perceptible delay that allows you to disambiguate. And that seems to work pretty well. Another approach that I'm excited about trying is actually doing both optimistically and then retroactively picking one once the disambiguation becomes more clear and rolling forward with that and unwinding the other one. So you can imagine with this pinch approach, you start doing a pinch slash roll it's ambiguous, and it basically starts to zooming imperceptibly and scrolling and perceptibly. And then once it becomes clear that you've done one or the other, it unwinds the thing that it wasn't, you know, zooms out slightly, for example, and then keeps doing the thing that you were doing scrolling. For example, 

Speaker 1 (19:16):
We're actually already doing some of that, um, in a similar, in a similar problem. So the same way that I was just talking about you can two fingers crawl anywhere on a board. Um, you can also drag any card on any board with one finger and we deliberately, as you just pointed out, we deliberately wanted to make that instant. So most apps work in a way where you hold your finger down on something, and then it sort of enters like maybe slightly lifts and enters into a moveable state, and then you can drag it around. Um, and that's exactly the thing that we didn't want. And I think one thing that makes news very unique that is like ultra responsive. So as soon as you set your finger down a card and you start moving it, you can even have your finger do a movement. 

Speaker 1 (20:02):
As you set it down, the S the cat will start moving with you. And so the problem with then the two finger scrolling here is that when you do want a two finger, you do want to use two fingers to scrawl. And in that case, we don't want to move that card. As you set down your two fingers, inevitably, one of the two will set down first, because again, we're humans, not robots. So even if this was a fraction of a second, that first finger that comes down and moves by one pixel will trigger the cat movement. But then the other thing that comes down and then the system actually recognizes, oh, it's a scrawl. And it actually cancels the cat movement. So you might sometimes if you do it very fast, and if your, your first finger goes down noticeably earlier than the other one, he will see your, your cart start dragging in and jumping kind of animating back into place where you picked it up from, and then the scrolling kicks in. So we're using that trick already a little bit in the app, but it's quite cumbersome to implement that. So hope, hope eventually we'll have more of a unified approach for this kind of thing. 

Speaker 2 (21:07):
Can you talk a little bit about what the overall framework here is? Um, is it essentially a giant case statement or a series of statements, or is it more of a state machine, or what does that, and what does that look like? Yeah, 

Speaker 1 (21:20):
It currently isn't really a very cohesive system, um, because of how some of the components interact. So you still want to be able to kind of give individual components the ability to control themselves, basically, without writing this, this global gesture handler component 

Speaker 2 (21:43):
To control itself here, you're talking about that. There's not one entry point for someone touched the screen. It's more, you want to attach a, a snippet of code or a piece of functionality to say a card, and it sort of knows. So to speak how to, um, how to manage touches that it re it receives, and that can be somewhat independent from what another card does. 

Speaker 1 (22:04):
Yeah, exactly. So for the cards, actually, um, we do have a bit more of a global approach because of how much the car dragging interacts with other things like zooming in and out of ports while you're dragging and cart along. 

Speaker 2 (22:16):
Is this the card card carry maneuver? 

Speaker 1 (22:18):
Yeah. This is the card carry maneuver that made everything so difficult, difficult for us. 

Speaker 2 (22:23):
Okay. Well, in the backstory here is it's pretty critical, right? There's, you know, if you're inside a board and you have one or more cards you want to take elsewhere, you can, um, you can stick it in the inbox. There's kind of, you know, maybe you can use copy paste, but that's kind of a hassle, really what you want to do is grab it and then navigate to your new location. And in fact, that's how it works and pumps, we call it the two K two handed card carry. So you can put your finger down. You've kind of picked up. So to speak that one. And then if I pinch out with the other hand, I'm essentially, now I can freely navigate around and I kind of keep this other card in this floating state. Um, but that's the thing that doesn't work if the gesture handler is attached to the card itself. 

Speaker 1 (23:04):
Yeah, exactly. Cause the gesture, uh, in order to be able to carry the cart into a different space, we basically have to detach it from its parents. So before it was living on the sport, and then if you had a gesture, recognizer attached directly to the card, you can move it around the board. But as soon as you put it to a different parent, the gesture recognizer actually cancels and you basically lose that gesture. And so in our case, in order to be able to carry it to a different board, we basically have to put it, um, on the top level hierarchy, basically attach it to your window so that you can zoom potentially many levels deep or further up your hierarchy until you find the board where you want to put that card and let go. 

Speaker 2 (23:52):
So many of these things that are challenging is because muse does come from a sip, a different set of product design principles. And one of them is this certainly the spatial zooming, um, interface, but also that we want to maintain this illusion of a continuous fluid space. Um, I think with many other kinds of applications, you have the sense of going to different screens or different pages. And you know, that when you go to, when you, when you navigate to that new screen or page, all of the kind of stuff that was on the previous screen just goes way or isn't relevant in this new place. And I think that's fine for a music player or something like that, but what we've tried to create this space where you have this big workspace and you can move stuff around freely between it, but then the kind of libraries and the API APIs that come with, certainly the iOS system, or I think any kind of UI system is just not built. Uh, assuming that assuming you want to do something like that. Yeah, 

Speaker 3 (24:48):
This is, this is an aside, but I really like that. There's no loading screens in muse. You don't open documents or load them. They're just there when you look at them. And that seems obvious, but when you go back and use an app, we are constantly loading documents, waiting for them to open. It's just a totally different experience. So I think it's worth the effort that we go through on the technical side. 

Speaker 1 (25:07):
Yeah. And, uh, you know, not, not least of that, um, the, the sort of challenging model that we chose from use, which is also something that makes it very unique as this, like you're, you're basically floating through space and you're zooming deeper into your hierarchy. And all of this is like a perfect illusion of seamlessness when it's actually not seamless at all. Basically every new bot that you load has to be rendered by the system, it has to be, you know, loaded into memory. And we, there are some tricks we're using there, but it's a, it's certainly not, not easy to keep up that illusion all the time. 

Speaker 2 (25:44):
Mark, I think you've made the comparison to video game development at various points. And this does actually remind me of, you mentioned loading screens, video games with big continuous worlds, which is, I think pretty common in today's, um, kind of open world games. This actually has a similar technical challenge that you don't want to interrupt the players movement and, and give them a now loading screen that really kind of as a kink in the experience or, or removes that illusion of being one continuous world. But in fact, when you have this huge world that can't possibly fit in memory, uh, you do need some way to handle that. And I think there are similar. I think I feel like a lot of the tricks that we've landed on to make this work, uh, for muse actually would be quite right at home in the video game world. 

Speaker 3 (26:30):
Absolutely circling back to gestures. Then perhaps we can talk about gesture spaces. So this is the idea of the kind of set of gestures that is possible on an app and the, that you can do with that. We found that to be a really interesting challenge with muse, the, the set of things that you could do with your hands or the pencil and the actions in the app that, that maps to. And one of the reasons this is so challenging is it tends to be much more constrained than a desktop app. So on desktop, you have the mouse, you have the two buttons, you have the mouse scroll, we all, you have the whole keyboard and you typically have the menus and the pattern of a right click menu or a press and hold menu. Whereas on mobile now, traditionally you just have like basically one finger and with a muse, we're trying to extend it to you have 10 fingers and the pencil, but it's still quite limited. 

Speaker 3 (27:21):
You don't have, for example, a menu where you can just add a bunch of stuff as you have more functionality in the app. So whenever you add a new feature, you need to find a way to invoke that with your hands, which isn't easy because there's a quite limited space, um, to draw from. So, so that, that, that results in a few concrete challenges, one is you need to come up with particular gestures. So an example for us is we needed to find a way to, um, pick the color of ink you're using. And for that, we have the swipe from the edge of the screen gesture, which is, I think pretty novel. Like that's not something that's built into iOS. 

Speaker 1 (27:56):
Yeah. The, the swipe from the screen with your pencil gesture is actually quite a harrowing thing. That's still an ongoing problem for us. So iOS actually does have a deliberate, I think it's called U I H swipe, gesture, recognizer, or something like that. So that, there's a way that you can attach a gesture listener to only swipes that happen from outside the device, into the screen. And I think iOS users that for all of their system wide thing, like you can summon the doc from the bottom, or you can navigate back by swiping from the left. Um, and I was, when I was initially implementing this menu, I was like, oh, great. We'll just use swipe, gesture, recognize like that. And we make it fire only for the pencil because notably those gestures, that system by gestures and iOS don't work with the pencil, you can summon the dock with the pencil, or you can pull in the control center with your pencil from the top. So I thought they would just be up for grabs those gestures, but unfortunately that edge swipe gesture, as it does not work with the pencil, 

Speaker 3 (29:05):
Surprisingly often we've run into that sort of edge of the map on the iOS API APIs, uh, because we're doing things that are quite unusual and muse. 

Speaker 2 (29:16):
And what do you do for testing and debugging this stuff you've talked about the simulator, but that's pretty poor for, uh, for this kind of thing. There's obviously you have the physical device there. How do you test this stuff? 

Speaker 1 (29:28):
Yeah. So testing gestures, um, is, you know, obviously a little bit harder than, than other debugging in some cases, because you can't just put a breakpoint in the middle of a gesture to see exactly what's going on. I, you can, but then you basically, when you then focus your attention back to the screen and to try to see what's going on, you have to lift your finger from the device that you were just testing on. And then once you, once you continue execution that Jen, that gesture will have ended to what I usually do is I put a lot of locks. So if I'm trying to disambiguate some gestures and often it's like very finicky, like which one fires first and then went, what, like what finger went down first and was it on a card or on the board. Um, and then I just go manually through those locks and try to try to figure out, um, the sequence in which things are happening and where I can, where I can intervene and tweak things. 

Speaker 1 (30:24):
Another thing that we, uh, that we use internally for debugging is that we have a little system that actually visualizes your touches on the screen that often helps to kind of explain to other people in the team. Look, when I'm doing this gesture, um, something happens that shouldn't be happening and there's a way that we can activate, um, basically little blue circles showing up around where your fingers are and little, um, and the different colored circle for your pencil. And then it's really easy to kind of record a video or do a screen-share where you show your, um, your peers, what exactly you're trying to do and what, where, where exactly your fingers are when certain things happen. So that's, that's kind of been a useful team team debugging feature, I would say, 

Speaker 2 (31:11):
Yeah, by sharing a video, you're showing a not only a reproducible case, but then you can even kind of slow. I find it useful sometimes to slow down the video or posit to, to figure out exactly what's happening there, where, and can, can make it more reproducible. I'm sure we can get more sophisticated with those tools over time, but yeah, those colored circles have proved combined with the screen recordings have proved remarkably useful for us in testing. Uh, you mentioned earlier the, uh, the operating system gestures like summoning a doc from the edge, uh, talking about the stylist from the edge reminded me of the, uh, take a screenshot by going, uh, stylists in from the, um, one of the corners. I wonder what happens in the case when we end up colliding with OSS system gestures, for example, we had some, some capabilities in the app when it was more in kind of beta prototype phase, uh, that did involve dragging up from below. 

Speaker 2 (32:07):
And those would get often interfered or, or had a bad interaction with the, uh, with the O S someone, a doc and notably, I think when we started working on the app, it was before the doc had been introduced. So that gesture to some of the doc didn't exist, but later it became totally foundational all now, iPads and I phones don't have home buttons to take you to the home button, you swipe up from the bottom, but then we basically had a swipe from the edge of the screen and specifically the bottom gesture. And that was colliding with that in a pretty bad way. And we, we basically had to make a, make 

Speaker 1 (32:42):
A change there. Yeah, well, I think the general rule is here, um, that the operating system wins. So this, this is a trend over the past couple of years where, where, um, dos has actually has been taking over more and more gestures, particularly around the edges of the screen. And in many cases for apps, that means they'll just have to change their gesture system. There is a way that you can, you can basically override these gestures once, uh, and tell them that, you know, I actually want to get the swipe first. So this is something that we, we tried out when we had the, the thing that you were able to pull in from the bottom. Um, you can tell the system to defer it's system gestured, let your, uh, your own app, get that gesture first. And what that does is that it, um, it makes your app execute the gesture, but then also brings up this like little arrow thing. 

Speaker 1 (33:37):
Um, and if the user actually, if the user's intent was actually to pull up the dark, then they have to basically do the gesture again on the arrow and then pull up the dog. But that makes a lot of users very angry. And I think rightfully so, if you, yeah, if you, if you learn how to, how to use your device and you kind of have muscle memory about around certain things and certainly, uh, such fundamental things, as, you know, switching an app and pulling up the dock, then you don't really want apps to interfere with that, or kind of override it with their own default behaviors. So you basically just have to cave in 

Speaker 3 (34:11):
Yeah. And there's a risk here of major gesture space reflow. So I mentioned how we're using basically all the gesture space that we know of for the app. It's all packed with our different features and functionality. And so if the OSTP takes away just one, you could have this musical chairs situation where one of the features of the app doesn't have anywhere to set. And so then you need to figure something totally different out for your gesture space, you know, open up a whole new room, for example. And we've gone through that a few times where we were just short of the degrees of freedom that we needed. So we needed to basically rethink how all of our gestures work 

Speaker 2 (34:47):
Just recently, a friend of mine was learning the terminal, the Unix terminal. And in the process of doing this, this was on a windows computer, uh, was surprised to learn that the copy command does not work. So they're used to pressing control C, but it turns out the control C has a long history, well predating the existence of copy paste buffers, uh, to break out of a program in the Unix command line. So typically these terminals on the Atlantics and, and, uh, uh, windows will basically take over that control C because they need it for the sort of B for the historical compatibility. And in fact, users are quite used to that as a way to break out of a program. But then if you're expecting that that's copy, which is a absolutely crucial, uh, capability that people rely on all the time, uh, it's quite confusing, distracting, annoying that that gets blocked and you need to use essentially another key command or another way of doing copy. So that sort of thing has existed since time and Memorial, but maybe iOS and the iPad in particular is such of a quickly evolving a new space. And so we're trying to push the frontier, but then the operating system makers also trying to push the frontier. And then simultaneously of course, as we explore the space, the likelihood of collisions is reasonably high. 

Speaker 1 (36:05):
And I think we're already trying to do a lot of things differently. Um, but we can't possibly overload the user with too many weird things. So in some cases just doing the standard thing is probably also a good idea. 

Speaker 2 (36:17):
We're definitely pushing right up against the ceiling of number of weird things for the, for the user to learn. 

Speaker 3 (36:23):
So Julia looking forward, what are you excited to try in the gesture system? 

Speaker 1 (36:28):
So I'm actually still kind of flirting with this idea of, um, something that you, you referenced earlier. I'm talking about this, uh, this UI con talk by Shannon Hughes, where she introduced this idea of actually building an entire state machine that manages all the gestures and your app. So that way you, you have one centralized place that always knows about what's going on, and what's possible to go from one state to the next. So if you wanna, if you sat down a finger on the screen from there, it might be possible to go into a pinch or into a draft card. And the state machine would handle all of the valid states and state transitions. And that way you could have a more deterministic and consistent approach to things, and you don't have to scatter different, different, um, dependencies across different components of your app that, uh, have to check, am I currently dragging a card? Do I need to cancel that drag in order to start the scroll? Um, so I think a bit more centralized approach, there could actually be interesting, but it would also be a lot of work. Um, so currently we haven't, we haven't made that a focus yet because what we have is working pretty well, but if, if, if we ever get bored or if this ever becomes a huge issue, I think that would be something that I would be excited to try 

Speaker 2 (37:50):
While there's way more to talk about here. Since we've invested a huge amount of time into this gesture system and certainly will going forward, perhaps we'll leave it there. So if any of our listeners out there have feedback, feel free to reach out to us@useapphqontwitterorhelloatnewsapp.com via email. Love to hear your comments and ideas for future episodes. You'll have very glad that you're, uh, working hard to make it possible for muse users to have this fluid and powerful interface for interacting with their ideas. Thanks. Yeah, 

Speaker 1 (38:26):
It's been, uh, obviously obviously a lot of fighting, but also a lot of fun.

