Speaker 1 (00:00):
I do you think there's a distinction between velocity and vitality? That's important to make, right? Like a good book can go viral. A podcast can go viral. It just will go viral slowly, a slow spread. And I think that's actually kind of a goal is to have potentially like a low velocity, a high virality. 

Speaker 2 (00:21):
Hello and welcome to Metamucil use as a tool for thought on iPad. This podcast, isn't about muse products. It's about views the company and the small team behind it. I'm Adam Wiggins here with my colleague, mark Brannigan, Adam, and our guests to bias rose Stockwell. You're there. And yes, uh, 

Speaker 1 (00:40):
Mark and I just recently did an episode on video games and how Mark's thesis that video games are where technologies kind of emerge first and later, they make their way to productivity and enterprise software and that sort of thing. And I feel like our meeting, which was in an online game, a text-based mud, they called them back then in the nineties, it was a good example of this. We knew each other virtually before we ever met in person for, I don't know, a year or more. And nowadays we take for granted that you meet people and even have great friendships. I think in, you know, your slack channels or online conferences or colleagues you've only ever met through video calls. But I feel like that was quite unique for the time. Truly Julie, I remember your characters on them. You had an amazing automation system in place for your characters. 

Speaker 1 (01:30):
You just, yeah. You crushed that game. That's right. I totally forgot the mud world because it was all, text-based almost kind of had a Unix style in that sense you would type commands and then you would see these descriptions. What was happening with the action is very scriptable because you could make what were called triggers, where you would essentially say, okay, if, when you see the word and we see someone's name react in this way, or when you see this happening, you could cause it to trigger another command people would do in some cases, very sophisticated scripting. I used a thing called Tintin. I think I was pretty simple with it. Some glad to hear it seems so impressive, but I think that did probably influence a lot of my thinking on kind of end-user programming, personal scripting world of things. Yeah, it was super impressive. 

Speaker 1 (02:13):
I'm like, this guy knows what he's doing out here and to buy us, maybe you can tell us a bit about your background, what you've been doing, kind of in the post mud time and then leading you up to what you're working on now, which connects to our topic today. Absolutely. Well, thanks for having me guys. I really love what it is you're working on and I always appreciate your analytical and pragmatic perspectives on the world. Understanding things in a more precise way. You know, we've known each other for quite a long time. I feel kind of honored that you have known me through several phases of my life, different chapters of my professional endeavor, which had brought me here. But when I was really quite young, I went and lived and worked in Cambodia for ages about 23 onwards for about six and a half. 

Speaker 1 (02:59):
I lived and worked on the ground in Cambodia, crazy wild story for how I got there, but essentially met a monk. When I was traveling through Asia who was looking for help in rebuilding, this irrigation system had been destroyed during the civil war. There got sucked into this project. I thought it would take maybe a year. I ended up staying there for six plus years, rebuilding this big reservoir that affected farmers and help them rebuild after this very, very problematic time. And just was very, very interested in what I can do to most help people figure out how to improve their own lives and ended up rebuilding this irrigation system. Getting interested in scale, interested in the motivations behind helping people help others more effectively, which ended up once I finished the project in Cambodia, I ended up coming back to Silicon valley where I grew up and started working on various projects to help people connect more effectively to humanitarian causes. 

Speaker 1 (03:58):
And this was between 2009, 2012. And it brought me to the world of online advocacy. And really this is the earliest days of social media at scale. And I was part of this cohort of designers and technologists and developers and documentarians that were really doing their best to try to motivate people, to capture altruistic action from the largest possible audience, right. There was this promise around these tools that was very kind of intoxicating at the time. This kind of inherent goodness that could come from connecting humanity. And there's this thesis, this broader thesis. I think that was implicit at that moment, which is, you know, if you can connect the world, like we can solve the problems of the world, right? You can just make people feel if you can make people feel the poor people in India, the people that are struggling in Southeast Asia, if you can really just connect people to the feeling effectively, then you're going to come much closer to solving those problems. 

Speaker 1 (05:00):
And that optimism was very real at that time. And it bled not just into the advocacy world, but I think it was very much a part of slope and valley at that time, too. So I worked on a bunch of campaigns that were really trying to capture virality and maximize human attention and get people involved with causes, get people to feel. And that was its own very special era. I worked on a few campaigns that reached millions of people. It's a very exciting time. And the tools enabled that in this very special way. And that was probably the cohort. And then people similar to you have many of the same people that were early at these companies that believe that these tools were fundamentally good for the world. And you know, in many ways they are. But I think that, you know, as we're saying, there are questions about some of their byproducts. 

Speaker 1 (05:45):
So fast forward a few years, I was working in New York doing design and management consulting, basically helping the executive teams of very large media organization, kind of storied and traditional journalistic institution that you would know if I named it, that I was working their executive team as the bottom was falling out of their business. And they were trying to figure out how to make money in this new media environment. And I was watching them as an institution begin to make decisions that reminded me fundamentally of types of attention capture tools that we had used in the years previously. And they were making decisions that were very much based on trying to utilize this new media environment that many of my friends had built in such a way that I think we would recognize today is problematic. But at that moment in time, we didn't, they were really starting to change the editorial tenor of the stories that they were making of the tools they were using to capture attention. 

Speaker 1 (06:47):
And it was changing the editorial bent of stories of content and pushing it towards the extreme. And this was in 20 14, 20 15, you know, just the years leading up to, I think our great awakening, some of the problems associated with this stuff that came in 2016, but they were really fundamentally changing the tenor and the content of stories to capture more attention using these tools and these strategies. This is what maybe nowadays we talk about as the classic clickbait titles and yeah, emotional activation that's designed to in a very short time, just get you riled up or activate some more primal part of your brain. And maybe that ties to your nonprofit work as well, which is also about emotional activation. But here you have this media environment where they have a very brief time to capture your attention. And they're just basically motivated to optimize for these headlines that push these buttons and activate you emotionally, even if that's not sort of good journalism or really a healthy information ecosystem. 

Speaker 1 (07:50):
Definitely. Yeah. And it's not just headlines. The headlines are maybe the most visible things. The stuff that tends to be a little bit more pernicious as the editorial decisions that are made around particular stories to cover, like what the cover, right? Journalism is this kind of important function as it is having three different fundamental pieces to it. One is the basic verification of facts, right? It's like, did something happen? Did the event happen? Did it not happen? Right? The next layer up is selective facts. Like which facts are actually important for us to pay attention to in the world. Right. And the top one is really like, why does it matter? It's that editorialization like, why is this important for us to pay attention to? And what I felt like I was watching in real time was that the selective interpretation of facts that the sourcing pool that editors and journalists were using to start trying to kind of find nuggets of stories, they started to trend towards the outrageous, they were finding this stuff that would make people the most mad. 

Speaker 1 (08:46):
Right? You might see this in a headline and a story in which the headline will be. People are angry about X, right? Well, people are angry about X while this is important. I should read why they're angry about X. And then you look at what they're actually sourcing for the quote unquote, people that are angry about X, and it will be a Twitter user, maybe two that have, you know, sub 20 followers that a journalist was able to kind of go in and find they kind of spun a story out of almost nothing online and wrote a whole article about it, which is terrible. If you're trying to get a proportional understanding of what people are actually angry about. And that's just one strategy of many that are now available to every journalistic institution or traditionally journalistic institution. For me, the first article of yours that I read that I think got a good bit of traction was titled, this is how your fear and outrage are being sold for profits, where you kind of broke a lot of this down. 

Speaker 1 (09:46):
And I think nowadays is part of the mainstream discussion, especially with something like, for example, this Netflix documentary, the social dilemma. Yeah. I think we have more cultural awareness of this now, but at the time for me, it was a real, eye-opener even being someone that was in technology that you were kind of breaking down the mechanisms, just sort of shining a light on exactly how you're being emotionally activated or even emotionally manipulated and why that's good for the use media companies in this new era. And then you went from there onto a series of other articles, including the dark psychology of social networks. So I think it was a cover story in the Atlantic with Jonathan Haidt, is that right? Getting right with a high-profile author, which is a whole other, probably interesting topic to discuss your latest article, which is how to stop misinformation for it gets shared collaboration with Renee de Resta on wired, which talks a bit about this kind of friction. 

Speaker 1 (10:38):
And so to me, these paint and you've written other stuff, but if you read the three of these and of course we'll link them in the show notes, they show a building up of what feels to me like a thesis or a sense of trying to understand or grapple with the societal effects of this new information technology that defines our world now. Yeah. I feel like that's right. So the second piece, you mentioned the piece with Jonathan height at NYU, he's a professor of moral psychology and we did a, almost a forensic unpacking of what happened between the years of 2009, 2012, in terms of the feature sets that were implemented at these various companies invented then copied then propagated across our traditional social media ecosystem and what that did, what these specific features did. And it's a few features that we very much understand today as being kind of core to our information exchange, but having a simple one-click share to send pieces of information out to your entire network likes, right, like fundamental likes and visible metrics associated with that content. 

Speaker 1 (11:51):
And then the ability to algorithmically sort of feed those kind of three pieces of the puzzle have dramatically changed the types of information that we are now seeing on a regular basis. And like each one of these features and themselves are great. I appreciate about that. The ability to reach a massive audience, I appreciate the signal that comes from knowing whether or not people are liking a thing. And I appreciate the ability to curate the crazy massive stream of information. That's not coming my way, but each one of them has kind of conditional failure modes that I think we need to understand and reckon with because having access to what's essentially kind of the brainstem of humanity now, right? If anyone can put something out there and anyone can make something go viral, there are tendencies within the system now towards what Daniel Kahneman would call system one, right system, one thinking, being this emotional reactive impulsive kind of instantaneous fast thinking, right, which is one of these partitions in our brains and the other being system two, which is this more reflective, deliberative, slower processing, and thinking that the entirety of the architecture of the internet in its current form are built for maximum speed and plurality it's orienting towards system one. 

Speaker 1 (13:15):
And that I think can be seen in so many institutions and so many changes. And so much of this like Geist of our exchange now is in this kind of emotional space. Yeah. And I think that's a great tee up, not only for our topic today, which is the promise and peril of social media, but also what the muse tie in is because a lot of what we talk about on this podcast is product design, but it tends to be more tools for thought productivity kind of more private things versus these public sphere, you know, political and social discussions. But I think there is when you talk about that system, one system, two brain, a big part of what we want to see Mews do, or a part of what our mission with our product and the company is, is to help us all activate our system to brains more reliably than when we realize there's something important. 

Speaker 1 (14:04):
We need to think about whether it's in life, work, social issues, what have you, that there's a way to kind of remove yourself from that energy and those outrage circles, or even just the heightened, emotional more primal state and go to a more thoughtful, reflective, slower thinking, because we believe, at least as complex as the world is today, you sort of need that in order to really make good decisions. Yeah. In addition to the system one versus system two access, I think there's an access of Denovo ideas versus remixed ideas. And I think basically all good ideas are social. The remix they're transmitted from other people and developed that way. And obviously social media is an incredibly powerful technology potentially for facilitating that. And some use the product right now. It's quite single-player and it's focused on developing content. You bring in content and you build from there. 

Speaker 1 (15:00):
But I think as you look at the broader process of thinking and developing good ideas and coming to useful conclusions, the social ecosystem is so important. Yeah. And I have my own, I think there's a lot of creative people do Twitter. Certainly my social media of choice. I've heard it described as sort of the social network for ideas. And if you're a person that's looking to seed ideas in your work and your life, I think Twitter is the right place for that. But it has these two sides, which is, uh, can be a source of incredible ideas, inspiration connections with new people. Certainly that's where my professional network is. I've met many amazing people, including a lot of the guests for this podcast, but have just had the seeds of good ideas. So often, I mean, there's a reason why we want to add a lot of media cards to be used. 

Speaker 1 (15:47):
But one of the first ones we did was a Twitter card precisely because bringing in a thought provoking tweet as a foundation for some deeper thinking is a very natural thing. But the flip side of this is what you're talking about. These loops for journalistic outlets and individuals as well, seeking that the sweet, sweet dopamine hit from those likes. Right? And you discover that those controversial or outrage generating things or things that just do those emotional system, one activations, get you more of that positive feedback. And so then you're sort of inclined to do that. And it just creates a setting where you have these kind of information pathologies and negative loops. And yeah, it sort of is very counter to the thoughtful, having good ideas focusing on your work. So there's this duality, there's these two sides of it that I feel are equally strong. 

Speaker 1 (16:39):
Yeah. I think that's really important is you spoke about how the generals, that Geist has kind of absorbed some of these memes about the internet being a place that prioritizes outrage that people tend to kind of understand the problems associated with this. Now there's a lot of great content out there. And a lot of talking heads that speak about the ills of technology. I do think that the hyperbole around it is actually kind of a big problem. I think that if you're too hand-wavy about what the problems are, then it actually doesn't help us solve them. It doesn't help us build better tools. Doesn't help us fix the tools we have. I think it actually just is pretty detrimental to the conversation as a whole, right? So there's this progression where I don't know in the mid late two thousands, it was the sense that this up and coming new information world was unmitigated. 

Speaker 1 (17:30):
Good. Kind of a sort of exuberance that in hindsight seems naive than sometime around the time you're writing these first articles, 20 16, 20 17, 20 18. There's a few folks like yourself that are kind of raising the flags and society's struggling to figure out, wait, what happened here? Things are changing maybe in a way don't like, it seems related to this new technology, you know, what's going on and then come to today. And again, I do feel there is a more mainstream sense that, okay, social media is this powerful technologies. Certainly these internet giants in general wheel, a lot of power. It does seem sometimes boiled down to a mark Zuckerberg as the devil. If we can get him from in front of Congress and slap enough regulations on him, then everything will be okay. It feels like a vast oversimplification that feels like this is a tomorrow. One of Mark's way of talking about things that society needs to metabolize the change. 

Speaker 1 (18:23):
And that's going to take some time. We need to thrash around and try some weird stuff. And maybe some of the solutions are governmental. Some of them are technology products, solutions. Like some of these ones you've proposed in your various articles here. And some of them are personal, right? How we choose to cultivate our information diets and make good choices that will allow us to get the most out of this brave new world of information without being maybe sucked down its worst rabbit holes. Yeah. I don't know if either of you had a chance to catch the tech hearings that happened a couple of weeks ago. I feel like there's one of these every month. I feel like. Yeah, totally. I seen some of them for understanding what happened there. I rely on people making dunk tweet jokes a hundred percent. You're part of the problem matters. 

Speaker 1 (19:06):
And then I extract what happened based on that. Yes, it was very clear to me. And I took notes of how many different members of the house had their own personal grievance about technology. I made a distinct list of 18 different areas that were very much not related other than the fact that they involve Facebook, just this whole kind of panoply of different grievances that they had. And what came to mind for me is this, this is not just technology's fault, right? Like we're just inhabiting technology more. And so we're bringing all of our problems with us, right? We're now just living in these digital spaces more and more and more. So we're bringing a huge portion of these problems with us now that doesn't reduce the importance of focusing on the tools and the specifics of the tools. But I think it is important to remember that humans are complex and our problems are complex. 

Speaker 1 (20:00):
Naturally. If we add a whole new layer of kind of virtual existence onto that, we're going to end up with a bunch of new problems and also all of the old ones manifesting in a different way. So I think it's just important to recognize that a lot of these issues are things that we're bringing into the fore with us. Yeah. Part of the concern would be technology is an amplifier for some of our natural sort of bad traits. We've made the comparison before to food, for example, where we came to this realization that a lot of fast food is actually quite unhealthy and feeding into a lot of health problems in the modern world, but that's because it was sort of optimized to push our sweet and savory and salty buttons because these are tendencies. We crave these things in our natural environment, but we found a way to kind of like supercharge what you get in a way that has these negative effects. 

Speaker 1 (20:51):
And I feel social media has a lot of that. For example, follower counts, I think is one I hear folks talk about, which is like, we have this natural status seeking behavior and getting a certain number of followers shows that you're, I dunno, important. You have prestige, people care about you or something like that. And so then that turns into weird status games, maybe of trying to game the system or just treating people differently based on their follower counts or whatever. And you know, one question there is, should we hide that? Because that basically just brings out some bad qualities in us. On the other hand, will people find other ways to seek that same? Because again, it's not the technology, that's something that's inside us as humans is to kind of be status seeking and just to elaborate here and to further motivate, I think there are a couple other dynamics that are making it even more important. 

Speaker 1 (21:36):
So this is baseline dynamic of there's like a very high powered social technology and people have always talked to each other and written stuff down, but now just like a lot more of that with my sharp edges. Okay. That's kind of what we've talked about so far, there's an additional dynamic that I would call the revolt of the public dynamic. This is the, I think his name is Martin Gary thesis. He wrote an amazing book about how basically people able to talk to each other outside the confines of traditional hierarchal structures is highly threatening to those structures. And this would be like traditional journalism, bureaucracies, higher education. And those institutions correctly perceive this huge existential rest of there's some kind of fighting for their own lives happening there. And then another angle is a lot of the most contentious stuff has to deal with politics. 

Speaker 1 (22:19):
And the reality is that politics, especially federal politics play a much bigger role in people's lives now than it has in the past. You know, government spending is what 40% of the economy plus all the indirect impacts like people are correctly interested in what's happening there. So these three things stacking on top of each other and creating huge stakes. So it's not surprising to my mind that people have strong feelings about this stuff. There's a great article. A Paul Graham article from it was 2007, 2008. The goal age of Paul Graham essays. Yes. About how to have better arguments online. And one of the anecdotes that he has in there is that people are now just accessing more information. There are more opportunities to collide online. There are more opinions that we're being exposed to. And because of that, people are going to be arguing more, right? 

Speaker 1 (23:10):
There's this kind of natural trend towards increased opportunities to disagree in a public forum. And there's certain things that happen in a public forum when we're disagreeing and the kind of ergonomics of that space or the design of that space online will push people towards a particular kind of disagreement or a particular kind of agreement, potentially depending on how that space is designed. One of the things that we speak to in our Atlantic piece is this kind of idea of what's called moral grandstanding. If you imagine us having this conversation right now, this is a great, we're having this conversation. This is over zoom. I can actually like, see your faces. We're having interaction, right? There's not an abstraction layer. You're responding to me. I'm responding to you. I can see your eyes. If I say something mean to you. If I have a disagreement with you, there's a desire that I have to reduce any kind of empathetic stress or sadness I might cause you, I'm not going to call you a name because I'm actually getting some kind of empathetic response from you and this physical stimulus that's coming back to me, this visual stimulus is coming back to me and the digital world that's hidden from view, right? 

Speaker 1 (24:19):
So you're abstracted out into this profile pic. If you even have a profile pic, you're just to kind of abstract creature out there, make sure even a creature in my mind, right? You might just be a bot in my mind that I'm angry about, but not only that, there's this additional layer in most of our social spaces online in which, I mean, you can imagine us having this conversation live with an audience of people around us, right? And like an audience of people around us live that were rating us. And there was a number attached to our faces. It was going up or down depending on the quality of our argument and what was actually happening in real time. I already feel anxious with that description. It would fundamentally change the content of our conversation in a drastic way. And it would not push us towards conclusions between us or attempting to find truth. 

Speaker 1 (25:09):
It would actually push us towards the approval of the people watching us. And that is what a large portion of our social media platforms are designed around right now. I mean, this kind of gets into that personal information, hygiene and personal information. Gardening is the device in your articles. You often are speaking essentially to the platform creators, you're saying, okay, Instagram or Facebook or Twitter here are features you could build, or maybe users could demand those features, but essentially change the tool in order to have better social dynamics. And absolutely I think the tool evolution we're seeing that happening already. Describe a lot of that in this kind of friction concept in your most recent wired article. And I'm sure that's going to be ongoing for a while, but for me, there's also this question of, to what degree I'm now working on a social media tool. 

Speaker 1 (25:59):
So therefore, what actions can I take in my life? And I'm curious how you see the balance between what we need to do here is kind of demand changes from these tool creators versus we can make choices in our own life. And for me, part of the value in your articles has been, I have a more critical eye or have more self-awareness about my own, how I engage with social media and how those things trigger my primal emotions. And with that awareness, I can then make maybe better choices about making sure I get what I want out of these tools and avoid the negative spirals. Yeah, absolutely. There's a really important balance I think, to be made for, you know, what the design is, the space versus the things that we can do as individuals. I do think there's a lot of things that we can do as individuals, fortunately depends on the particular type of problem that you're facing online. 

Speaker 1 (26:50):
But in general, if we're looking at it in the context of like these system one system, two dynamics, usually if I am triggered by something I see online, good to kind of mental model for me is to try to force myself in those moments where I feel the desire to basically like rage tweet back at someone or share a thing that I am incensed about. Right. And that's one of the big ones is just kind of propagating highly emotional content and you know, not all emotional content is bad, right? Emotions have a purpose in our lives, right? I think it's important to recognize action and bring us passion. I'm doing what I'm doing in my life because I'm passionate and have strong emotions about those things. Exactly, exactly. So emotions have reasons and they're kind of, you know, internal heuristics for us to determine what is important on a day-to-day basis. 

Speaker 1 (27:37):
It's like they're helpful directors for what we should focus on. And, you know, biologically and socially, they have kind of foundational adaptive purposes in our lives. The problem is that in talking to my application in these digital spaces, we're actually taking some of these emotions, which are kind of meant to pass through our bodies and just like be noted and then direct us towards a specific action. We're encapsulating those and little time capsules, right? We're textualize ING them. And then we're sending them out on their way into a network to go on and kind of trigger other people and have the life of their own. Is there ping-ponging around our networks and causing other people to be emotionally activated. And they kind of live in a semi-permanent state, even though initially an emotion is just kind of meant to pass through our system. Mm. I hadn't even thought about the sort of responsibility to others by passing on emotional content. 

Speaker 1 (28:25):
I tend to think in terms of, again, how can tools be improved, but then how can I make changes to my own information tooling and sort of managing those processes in my life. Mark and I talked about this quite a bit in our previous podcast on what I call the information age there. I think we were focused more on just the raw quantity of information that we're attuned through. Most of human history. We lived in time where information was scarce. And so we seek to get as much yet news. Like even the word news implies. Yeah. Like what's, what's the latest I want to be in the know I want to be connected. It's important to know what's going on. And now we live in a time of, in such incredible information, abundance that actually the problem is curating and calling all that out. But mark, I'm curious, you know, expanding on what we talked about there. 

Speaker 1 (29:12):
I feel like you have a pretty good set of approaches for cultivating your own information ecosystem, putting it together with some of what we're talking about here. What are like techniques that work for you or you recommend for others? Yeah. There's a lot here. One quick thing on the emotional front with weird things coming at you on Twitter and stuff. I adopted a simple practice of whenever something gives me bad vibes on Twitter, I just block or, you know, turn off retweets or mute or whatever is appropriate to the situation. That's fine. Like no too short for bad vibes coming through of your own choosing, right? You have total control of what you see on Twitter. Just choose to close that stuff off. I feel like the immediate devil's advocate on that is there are bad things in the world and it's not helpful to not be exposed to problems like things that you disagree with or things that are bad that are happening. 

Speaker 1 (30:01):
It's things like bad faith and people being obnoxious and stuff like that. And it's the default that you continue to see people that you've previously followed. And for a long time, I just had this out. For whatever reason, I wasn't inclined to change that. I'm just kind of a Creek follow ease over time. But then I took more responsibility for my timeline and things are much better now. And that in general is a big theme for me with personal information management. Like no one's going to save you in particular. The algorithms are definitely not going to save you. If you just go to facebook.com and click on whatever's at the top, you're gonna have a bad time. Likewise, if you just go to like walking to post.com and read everything there, you're not gonna be very well informed. So you need to take a lot of responsibility for your own information ecosystem. 

Speaker 1 (30:41):
And that's why I like things like Twitter, podcasts, email newsletter, niche discords and Reddit's, these are places where you opt in to individual small creators curators, communities who you believe have good insights and relevance to you. By the way, I just realized to bias when you were talking about the three, like pathological features of social media, that podcasts are kind of uniquely resisted to them. So podcasts can't be retweeted. They haven't really been subjected to algorithmic feeding Spotify is working on it, I think. But yeah, so far. Yeah, but mostly they've been quite resistant. I think that's an important dynamic. We see that on the side of being podcasts, creators, or what have you, which is it's actually hard to market or spread a podcast and happily then click side. I'm very thankful for everyone who's tweeted out links or referred us to their friends. 

Speaker 1 (31:34):
But yeah, it's just a podcast can't go viral. That's all there is to it. It can grow pretty slowly and steadily and organically over time. And it's a downside in some ways, but actually really a benefit in the sense and probably why I'm drawn to podcasts often as a place to get more called system two inputs into my life. Definitely. I do think there's a distinction between velocity and virality. That's important to make, right? So like a good book can go viral. A podcast can go viral. It just will go viral slowly. It'll be a slow spread. And I think that's actually kind of a goal is to have potentially like a low velocity, a high virality thing in which people are like, oh, you should listen to this. You know, like a word of mouth recommendation goes a long way, especially for something like podcasts, mark, what you're talking about, you know, your personal curation systems. 

Speaker 1 (32:21):
I find that to be so critically important these days. I like to think about algorithms as kind of like a dog. It's something that is like an intelligence that has a very simple understanding of what it's trying to do for you. Right? And it's using limited inputs to determine what it is, is trying to serve you on a regular basis. So the same way you might train a dog, I aggressively, for instance, train my YouTube algorithm is constantly serving me up things on a regular basis. And probably once a week I have to go through and say, I don't like this. I don't like this. I don't like this. And as a result I get, and I use this kind of personal heuristic, which is like, how do I feel after watching this video? How do I feel after going through my feed here and checking in with yourself, taking a moment to recognize, I think there's like the regret test, very basic tests. 

Speaker 1 (33:12):
Like if you regret your experience on this tool, then you need to change the tool or get off the tool. Right? Unfortunately that is something that we can do. So I have aggressively called and trained my YouTube feed. And now most of the time, you know, again with this kind of weekly training regimen, it becomes a beautiful source of inspirational educational content that I really get a tremendous amount of value. Yeah. And I think on this personal curation stuff, you need to be really willing to go against the grain because the grains in the wood here are leading down to places that are bad. And in many cases, just misinformed or wrong, especially with like the kind of complex topics of the day for various systemic reasons, the stuff that you get and the prestige outlets. Isn't very good. And so you really got to go around and crawl and find the weird Twitter accounts. 

Speaker 1 (34:03):
I've joked that on many complex topics like the recent pandemic, a lot of the very best information on the internet came from cartoon, AbbVie, pseudonymous, Twitter accounts. I mean that 100% seriously. So you need to be willing to embrace that kind of weirdness in your own personal information creation if you're gonna want to have good outcomes. I think so what you're saying, speaking to there is really interesting because you're absolutely right. There's been both like a decline in trust to journalism at large, right? That's a big, big problem kind of we're facing right now is that we don't feel like we can trust historical institutions that we used to rely upon for this basic validation of perspective and worldview that, you know, we used to have kind of the forest to feed essentially, right. We had a forced algorithm, which was the major news and media. 

Speaker 1 (34:46):
That was our content three channels. They all kind of say the same thing. You can pick one based on whether you like the color of the presenters tie, but they're basically giving you the same information. Exactly. Like forced consensus essentially about what is factual and what is not. Yeah. And so as the internet has kind of detonated and exploded this traditional media hierarchy, it's also along with it that needed this trust network that we had, right. Trust in specific authorities. And for a lot of people that's laid bare. And especially during the coronavirus is this kind of proxy network of what I call reality anchors, right. We used to have kind of media anchors, which we looked for. We had the Walter Cronkite's and the Dan rathers, who we look to for specific points of reality. This happened, this didn't happen. Now everyone kind of has their own proxy network of reality anchors, right? 

Speaker 1 (35:36):
It might be the following there's the, or it might be the IDW intellectual, dark web thought leader. You know, it could be any broader group of humans, but those are really tremendously on display because in a way that they weren't previous. And I think that's laid bare a lot of the problems of hard at the systemic environment now because we no longer have a specific kind of source of consensus that we can look at collectively, we instead have this decentralized individuated network of reality anchors that we point to. Yeah. And it's super messy, which I think circles back to a lot of your work because a lot of these so-called reality anchors, they're not based in reality. And we're asking every individual at the comp with their own notion of reality and inevitably most of them are wrong and all of them are at least different. 

Speaker 1 (36:20):
And so what do you do with that? It's an extraordinary mess. Not an easy answer, right? Yeah. I find myself conflicted about, on one hand, the centralized force consensus as you called it. I think that left a lot of important viewpoints out in the dark and living in a world where I do have access to much greater diversity of opinion and ideas and perspectives. I think I'd once heard the, it was Megan. McArdle described the internet as a quote, free liberation front. And I like that because I'm interested in lots of weird things and weird things were sort of hard to find when you did live in a world of broadcast media, there's only a few channels. There's only a few newspapers and what have you. And I'm basically much happier in a world where I do have access to much nichier things, but then yes, we lose that shared consensus, shared understanding of reality. 

Speaker 1 (37:13):
And I would not have thought this previously, but I think in past years, I've seen where in a way there is something to be said for a whole society, basically agreeing about the basic facts of reality. Even if some of those facts are actually wrong, but if we agree together, we can at least make decisions together. Whereas if we have a totally different view of reality, we're paralyzed and an action maybe is worse in some ways than taking action on a false understanding that then perhaps in the future, you can correct as you realize. I don't know, for example, that the concept of the food pyramid is ridiculous. Totally. You know, if you look at history and if you go back far enough, you can see there's a correlation between our ability to organize and the larger and larger groups with our ability to kind of understand each other and share common cause cooperate, better share good information together, collectively that kind of thing. 

Speaker 1 (38:08):
Um, good information qualified to share information together at all, right. They share ideas together. And we kind of emerged from this fog of, we think about misinformation being a new problem, but we kind of emerge from this fog of misinformation that was actually really just kind of endemic in society, right? Like we used to believe crazy, crazy things. You just have to scratch the surface of history. You don't have to go back that far to see that there was just this kind of constant barrage of falsehoods or half-truths, that would pretty much become accepted truths, right? Come the accepted truths for a long period of time. You probably remember a handful of these from your childhood, maybe even right. In which like a Weegee board might some of the devil or, you know, a story about aliens or something. But we have this really strong consensus force consensus mechanism in place to kind of keep everyone on the same page. 

Speaker 1 (38:58):
But I think the natural state of human exchange is much more oriented towards misinformation than it is towards fidelity and truth. And I think that's important that what we've done with the internet is just kind of backspaced to a previous era in which misinformation was emergence and common, right? Yeah. That brings to mind to me what they called the yellow journalism era. We talked about clickbait headline titles, but in fact there was a version of that for when they were selling sort of one-off newspapers by the newspaper person, probably a young man standing on the corner, just yelling the headline. And there were these patently ridiculous headlines about, you know, we've gone to war or other things have happened because I just wanted to sell papers basically. And I think eventually journalism settled on a better model that was more kind of oriented around longer term useful truths rather than one-off attention grabbing headlines. 

Speaker 1 (39:52):
But yet to say that that's new to this era would be quite incorrect. I've been especially fascinated with that particular era of when we basically began to use advertising to propagate journalism, right? So the era of it was actually the era before yellow drones and there's this whole century in the 18 hundreds in which we kind of figured out that you could sell ads with newspapers. And in that process, you could also make a lot of money by making sensational claims. The New York sun was the first paper to do this in the early 18 hundreds. And they began their whole business and this whole kind of enterprise of ad based journalism by literally making up fake stories and propagating fake stories on the corner of the guys, Hocking it and getting people riled up about stuff. There was a story about animals escaping the zoo and marauding and killing people that they put in the stories, extremely irresponsible journalism that like caused a mob to go out and try to find these animals and put them down because they read it in the paper and they thought it was real, which is literally just a false, just entirely made up story that they wrote to sell papers. 

Speaker 1 (40:57):
And there's just after that, there was a story about bat people on the moon that there's astronomical observations of bat people living on the moon life on another planet that was a sensation sold a lot of papers and, you know, kind of caused this crazy confusion about life on other planets. That was a hugely popular hit. Yeah. That also makes me think of George Orwell and the war of the worlds broadcast and just, yeah, the power of media, particularly when it's new, I think maybe we build some kind of resistance or just awareness ability to separate and say like, well, because I hear something on the radio doesn't mean it's true, or I see it on the TV doesn't mean it's true. And maybe we're still in that era for a lot of these internet technologies where, well, certainly I hope people realize that when you see something on the internet, it may not be true, but maybe we haven't yet developed these antibodies or coping techniques for you see an outrageous tweet and you immediately reach for the retweet or the reply button. 

Speaker 1 (41:52):
And we haven't quite developed the better practices for managing. I think the word is anybody's. I think that's a great way of thinking about it as like a cultural antibody against the thing, the way that changed in that era in the mid 18 hundreds, it was through this process of professionalization that was actually also market driven. It wasn't like the government got together and was like, we're going to tell you what is true, what is not true? But it was this process of consumers learning that they couldn't really trust specific papers and they could trust other papers. Right? So in the New York newspaper scene, that was the sun and these other papers became kind of like trash papers. You know, you fool me once. I'll believe you fool me twice. I'm definitely not going to buy you again. Right. And one of the papers that tacked up the brainstem as opposed to down the brainstem was the New York times. 

Speaker 1 (42:44):
And they actually started to build reputation around actually reporting facts and being consistent about that and being trustworthy. Do you think about it as a little bit like an iterated prisoner's dilemma for us accurate information, right. In a world in which there's nothing but crazy, outrageous headlines month over month, you're going to learn to try to focus on the things that are verifiable and reputations come to matter. They actually are built over time. It does seem like the value of news is in it being true. It gives you information about how to navigate your world, you know, in the previous more previous civilization time news, that there's a dangerous bear prowling around, outside the village is really useful for keeping yourself safe, if it's true. But if you're spending a bunch of time avoiding bears that aren't there, you're just draining energy on something that's not valuable. 

Speaker 1 (43:32):
Now I will argue that a lot of our news and information consumption is fundamentally entertainment. Nowadays. I think the degree to which political news is essentially a kind of entertainment kind of following your favorite sports team kind of thing. And I think that's fine, but at least it seems like in theory, bad information or incorrect information, doesn't help you navigate the world and has a maladaptive effect. And pretty quickly you're going to see, okay, I made decisions in my life based on this incorrect information. Therefore my life went less well than it could otherwise. And then eventually you're going to want to turn that around and get true information. Yeah. I think that's an important point. Yeah. I think that's very much the case. Like I don't think we're going back to the world of mainstream journalism as the purveyor of verified facts, even as tastes like the New York times are already on the train of entertainment and aligned analysis, let's call it and they're moving even harder to that model now as they catch up with sub stack. 

Speaker 1 (44:29):
And so the question then becomes, where are we actually going to get these true facts, which are the sort of public good. And once the fact is out there, you can't charge for it. So who's actually going to provide it. And it's a very tough question. And right now the honest answer is that it's almost no one. So you kind of have to do it for yourself. And we're starting to see now people can fill this hole a little bit by becoming named individuals who have a secure reputation attached to them personally. And that way you can bootstrap up into something that you can trust in the way that you can't trust a large institution, because it can launder and transform reputations. Right? So it's pretty gnarly out there right now. And I think it's important just to be honest about that and to expect that you're going to have to navigate that. 

Speaker 1 (45:09):
And furthermore, that the whatever new equilibrium we reach, isn't going to look like the old world. So don't be ready for that too. That's a good point. The tools that we have now, and this might be a good moment to segue into my most recent article just quickly about friction. Just like there are ways that we can potentially reorient the system to at least trend towards accurate information versus falsehoods. Right? If you look at the trends of misinformation right now, it occupies the same reach as a traditional media broadcast, right? Some viral mistruths can go just as far and as fast to a wide audience as traditional media podcasts, where they can reach millions of people. You know, pandemic was a good example of that being that 12 million people that saw it. So potentially there's a concept of friction, which is basically throttling misinformation as it spreads through a network and kind of reducing just the inherent capacity for virality across the board, right? 

Speaker 1 (46:06):
So WhatsApp did this, they reduced the net number of possible shares. They reduced the number of groups that you could actually share a specific message to, right. And they just throttled that down. I forgot what the final number was, but they basically just, I think it was the throttle down of five groups from infinite groups, essentially. So you could before just copy and paste a message into as many different WhatsApp groups as you want it to be before, as you wanted to get your message into. And they just dropped that down, that fundamentally improved the speed at which misinformation could potentially travel, can still travel, but it just travels more slowly than is less impactful. And so I think that shifting things towards a slightly slower propagation system can actually dramatically improve the types of misinformation or responding to and, or the basis. Yeah. And I think as we've discussed, we've already seen some evidence for that in the different types of discussions you get in the different mediums. 

Speaker 1 (47:02):
So again, the ones that I find most compelling these days are podcasts and email newsletters and small Twitter accounts that you can't quote to a podcast. And that's in many respects, a feature, not a bug. I'm more optimistic about those types of structural approaches than content based approaches because the content based approaches is so fraught and you're back to the Oracle problem of what's actually true versus not. And I'm sure people are going to try that in, continue to try it. But I do think the structural approach is more promising. Definitely. And there's the problem of censorship and kind of the rights of saying what you could say, you know, Renee, the rest of my coauthor on the most recent piece, she has this other article and this lettuce thing, which is the freedom of reach is not the same thing as freedom of speech. Right. Which I think is a really important distinction when we're talking about this stuff, because yes, like you do have the ability and the right to say whatever you want, but virality is not a right. 

Speaker 1 (47:52):
And you don't have the right to reach hundreds of millions of people with what you want. We're looking forward to the future to us. What do you see the solutions potentially being? Is it individual information curation? Is it these companies that run these products, making these feature changes? Is it government regulation and involvement or is it something else where does a happier future for our relationship with social media, with information spreading with [inaudible]? How do we get there? Yeah, I really think it's a mix of both. It's both thoughtful design changes on the platform side to help us make better decisions and better defaults to have better defaults about the types of information we're regularly exposed to. And then I think as we're speaking this kind of cultural antibodies piece of it, it's like, it's really important for us as individuals to be aware of the information ecosystem that we're living within and to approach stuff with good faith skepticism. 

Speaker 1 (48:51):
That makes sense. And to really try to make decisions about what we're sharing consciously and to try to push ourselves, I think, to a more reflective state, as opposed to this more impulsive state. And I think that's a big piece of it. Quick anecdote about when I'm triggered by something online. I will invariably, if I feel the desire to reach for the retweet button or the rage tweet, I will take four deep breaths before I do anything. And in that moment, just that small act of taking four deep breaths will usually allow for my emotional reaction to dissipate in such a way that it feels like a much more healthy thing I'm about to say or act I'm about to make online. I think that just four deep breaths would go a long way in improving the type of internet we inhabit. If everyone could do something like that, couldn't agree more. Well, let's wrap it there. 

Speaker 2 (49:41):
Thanks everyone for listening. If you have feedback, write to us on Twitter at Misa HQ or via email, hello@museapp.com to help us out by leaving a review on apple podcasts and by us loving all the work you're doing on this. And if I'm not mistaken, you've got a book coming down the pipe here somewhere. My correct that's right. Yes. Book is due out next year. It's about outrage on the internet, how to navigate this new, strange digital world, how to make our digital tools better stewards of our humanity.

