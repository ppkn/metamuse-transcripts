Speaker 1 (00:00):
To us. It's very important that we design this all holistically. There is a lot of research, for example, on cryptography schemes that assume key management or on collaboration, product designs that assume the server can just read all the data. And in order for this to work with muse, we need all of the product design, the collaboration technology, the key management, the cryptography, the mental model of how people think about documents that all need to line up 

Speaker 2 (00:31):
Low and welcome to Metta muse use as a tool for thought on iPad. But this podcast isn't about muse the product it's about muse the company and the small team behind it. I'm Adam Wiggins. I'm here with my colleague mark Brannigan. Hey Adam and mark. I like to listen to podcasts in the morning, but I understand that you have a slightly more unusual and in-depth source of audio. 

Speaker 1 (00:55):
Yeah, this morning, I was actually listening to the real-time oral arguments in the U S Supreme court on their very important ACA slash Obamacare case. This is obviously a very big case for the U S and for many of us personally. And so I was keen to listen in and see what the judges were thinking. And this is notable because I think until recently, you couldn't actually listen to these broadcasts in real time. There wasn't the CSPAN equivalent for the U S Supreme court until I think COVID hit. And they started doing everything via audio, audio, you know, zoom or equivalent. And I think at one point actually they didn't release the audio to the public until quite a long time after the arguments had concluded, I think they did it every term or something, which is six months or something. And then more recently they changed it to be every week. And then now they release it in real time. And of course that's interesting as a interested citizen, but also it kind of connects to our topic today of privacy, because one of the ideas that we'll talk about is how visible or non-visible your work is while it's in process. Yeah. Privacy 

Speaker 2 (01:52):
Is a huge topic and it's something on our minds right now because we're making some product decisions for the sharing and collaboration features that will be forthcoming from use. So in the process of working through this as a team, where do we make trade-offs on things that necessarily results in a kind of a zooming out or you can't help, but to look at the larger context, which is okay, there's what do we want to do for our product? What matters for our customers? What's technically feasible? What do we value as a team? Then you zoom out a little bit from there to, okay, well, what's going on in the technology industry. Obviously this is a very, very big topic for the tech industry right now. And then you can zoom out even from there and talk about the society-wide impacts and you know, what does privacy even mean? What can we expect what's important or not important in terms of the, our lives as citizens, but also just as technology changes, we may need to adapt to what we can reasonably expect in terms of privacy. But yeah, as you know, I always like to start at the beginning of the definition or something of that nature. So what does the word privacy bring to mind for you mark? 

Speaker 1 (03:00):
Well, I don't have a super nice pre-packaged definition, but what's coming to mind is a sense of agency over who does and doesn't have access to your work. And you might exercise that agency by saying only I can read my personal journal, for example. And so that's private to me, or it could also mean that we are working on a project together. And so I want you and me Adam, to be able to see some work product, but no one else, or it could be that you want to share it very broadly. And that's your choice as well. So some sense of controlling who does and doesn't access your work. When I went looking 

Speaker 2 (03:32):
For kind of what is actually the definition here versus my own vague sense of what counts or doesn't count as privacy, which probably by the way has changed over the years. But the canonical one that's often linked back to is a piece in the Harvard law review in 1890, called the right to privacy. And they point out that some of these American values of right to life, right, to pursue happiness, right to secure property originally maybe meant something more practical. You know, property was your cattle, for example, but now you fast forward here. Now over a hundred years ago, they're writing and they say, well, wait a minute, we've started to recognize more of a spiritual nature of man's feelings, his intellect. And so maybe these rights that we talk about broadened a little bit, and the term property may include intangible things like your notes, like your thoughts, for example, they actually just use the phrase at one point, the right to be let alone, you know, maybe in modern phrasing, we would say the right to be left alone with the idea of maybe why you don't want everybody in the world to have your phone number is you don't necessarily want the equivalent of spam calls coming in. 

Speaker 2 (04:44):
You want to give that to people that you have this trusted relationship with, that you believe they're going to call you because there's someone you wish to speak to. You have an existing relationship, something like that. So that lends for privacy I've found. 

Speaker 1 (04:58):
Yeah, that's a very interesting definition because it gets at a problem that I've seen with a lot of the privacy discussion, which is it's very tempting to try to imagine or infer or argue for some very natural and fundamental right to privacy. Obviously, if you like privacy, you'll tend to do that. And I often see this in the form of privacy as a human right, or privacy is a natural, right? And I certainly think you can make arguments to that effect, but it kind of Dodges to my mind, the real fundamental question here, what are the trade-offs, where are the benefits? What are the costs and what work are we willing to do as a society to bring about those benefits? If we want them, because unlike something like perhaps the right to life, you know, you can get that just by not ending other people's lives, right? Whereas privacy is, is much more complex than you need to build a cryptography. You need different business models, right? It's much harder to grapple with another way to, I think about 

Speaker 2 (05:51):
At least for these intangibles, for information privacy, which is chiefly, what we're concerned with in our business and in the technology industry, generally, it's really communication is often the thing, you know, muse as a stance today, keeps all your information on the device there. And putting aside some threat vectors, like someone's stealing your device for the most part. That means there isn't so much to worry about. It's once you go to share it with another person or share it digitally over the internet, that's where things get trickier. And I liked your courtroom example because another one I had kind of sketched down was the social contract or the common legal protection that's given to what they usually call client confidentiality or patient confidentiality. So attorneys or doctors or therapists, the idea is that you are going to have a private communication with them, and you can expect both kind of a, from just a manners perspective, but also in some cases, legal protections for what you say there. 

Speaker 2 (06:48):
And that gets a little bit to what's relevant to our business, which is in one of those communications, speaking to your therapist, for example, but also sketching in your notebook. If I need to think about, okay, everyone in the world can see this either now or in the future, maybe that is going to consume some part of my brain, figuring out how comfortable I feel with that. Whether I want to alter what I'm saying or writing a little bit. And there's something freeing where I say, I'm in this communication with one other person or with my notebook only. So essentially myself for my future self. And I can reasonably expect that no one else is going to hear this or be privy to it. And that frees me in a lot of ways to be creative or to really open myself up. And it seems to be a common acumen experience that it's easier to truly open yourself up when you know exactly who's on the receiving end of 

Speaker 1 (07:39):
That. Yeah, totally. If I think about the benefits of privacy, to me, that's one of the three big legs, this, this idea that when you have control over, who has access to your thoughts, your work and your data, especially when that's quite limited, it encourages and allows creativity. And that might be creativity in terms of your personal journal, right? You're much more inclined to write something to share it with yourself, if you will, if you know, no one else is going to see it, but it could also mean you're doing some private brainstorming and that would be very different if it was just you and me versus if we were in a stadium with 50,000 people watching, right. And it's just, that's kind of how humans are. I think that's a big piece. And also it connects to what I consider to be the second big leg of privacy benefits, which is it allows you to manage communications. 

Speaker 1 (08:22):
So it might be the case that you eventually want someone to know something, but while you're working on it and you're preparing the communication plan, you don't want them to be processing all of the raw stuff. That's something that I encountered a lot as an engineering manager. You know, if you're working on an organizational change or something, right? You don't want people to be reading all of the raw discussions and debates about how that's going to happen. You want a clear and coherent and well executed communication plan. That's again, you need privacy for that. And just to mention what it is in my mind, the third leg, and we can perhaps talk about it later, but it does a protection from governments and other large concentrated powers. And for me, that's especially important with electronic data and communications to my mind, this stuff is so sharp because it's so easy for it to get replicated for it, to get distributed for it, to be intercepted for it to be ease dropped in a way that's just not the case with something like paper, you know, with paper, it's actually quite hard to make a billion copies of paper. 

Speaker 1 (09:17):
It's also very easy to reason about where the paper is going because it's in this physical world that we have a lot of familiarity with. We don't have the same intuitions or ability to reason about electronic data in terms of how long it could be persisted, how many people can see it and all the ways that it can be processed. So I think overall it makes this problem of concentrated data in the hands of large power is very sharp. Yeah. And I think in 

Speaker 2 (09:38):
The analog world where you're just thinking about who might be overhearing me in my office, or as I walked down the street, having a private conversation with a friend that you can kind of scope in time and impact. But when I put my photos, my notes, my, whatever it is into electronic databases that can be replicated potentially forever. I think of something like live journal, which was this journaling slash blogging site 20 years back, that was very popular. A lot of people, especially young people poured their very private thoughts and things about their lives into, under the reasonable expectation. That was only going to go to the few friends they've scoped to. And then in the meantime, it's been sold several times to several different acquirers. All that stuff is in there. What someone wrote 20 years ago in a database that's now in the hands of someone, very different from who originally it was in the hands of. And, and I think it's just, we don't quite yet have the capacity to actually reason about 

Speaker 1 (10:33):
Totally. And just to expand on this a little bit, this point to two other ways that electronic data privacy or non privacy can be very sharp. One is this time element where the data can persist and indeed accumulate and move around for a very long time. So we might say, oh, you know, with our current privacy practices, nothing that disastrous has happened. Well, we actually don't know because the half-life of this data is probably 50 or a hundred years. So we'll know. And I don't know, 200 years if it was actually a bad idea, but we can't really say that it wasn't until the, all the data has fully dissolved into the ocean or something. The other huge thing here is how humans are, or aren't part of the process. So again, with electronic data for collecting it for storing it, you just need to convince basically a few people. 

Speaker 1 (11:15):
It's a good idea. So if the NSA wants to read everyone's emails, they came into a few people at Google and Yahoo and that's basically it. And then they get billions and billions of emails. Whereas if you want it to eavesdrop on someone in the physical world, you got to pay someone, they got to go out, you know, to the rooftop and that's expensive. And if you have a ton of them, you have to actually convince all these people to do this every day and maybe actually have trouble convincing thousands and thousands of people to do this. So there's this kind of like human rate limiter, friction that you get, if you want to do wide scale data collection in the physical world, but you don't have it in a digital world. And this is another reason why I think the digital stuff is so sharp and potentially dangerous, feels 

Speaker 2 (11:53):
Like there's a parallel there to spam postal mail versus spam email, which is people sending you unsolicited advertisements and postal mail has always been a thing. It's still a thing, but it's limited by sort of physics and the cost of actually getting that brochure or whatever into your mailbox. And digital is just so cheap and so fast and so easy to do in this kind of anonymous unaccountable way that then it goes from being maybe a advertisement senior postal mailbox or a minor annoyance to being something that potentially overwhelms the systems of the internet and makes, you know, at one point threatened to make email a completely unusable service. And I think basically every kind of communication technology that comes online and gets substantial traction has to deal with that same kind of spam and abuse problem for that same reason. It's just so cheap to do that and try to grab people's attention through these automated and unaccountable means. 

Speaker 1 (12:51):
Yeah, spam is also notable because there was a very powerful technological problem. Basically people sending out zillions of emails for essentially free, there was also a very powerful technical solution, probability based spam filtering. And so there was this battle for a long time. I think we could say that eventually the spam filtering one, because they have access to all the leverage you get with electronic data that the attackers have. But yeah, that wasn't a preordained conclusion and I don't necessarily think we should count on that being the case with privacy. And I'm looking 

Speaker 2 (13:20):
At the things going on in the technology industry there, we have something like GDPR is a pretty big deal in Europe. That's been enforced for a few years. And then they've, I think are looking at doing more to strengthen it, sort of trying to give people more control over their personal data, more insights over what's being captured and when it's deleted and that sort of thing. Another notable trend in recent time is products, privacy focused products that have, if not broken into the mainstream necessarily have been pretty successful. There's something like the brave web browser that has built in ad blocking and essentially makes a privacy oriented pitch over using something like Chrome. And they just posted recently they had 20 million users, which is a pretty good number. Duck, duck go is a search engine that in many ways you can say is worse than Google in terms of results, but it's privacy protecting. 

Speaker 2 (14:12):
And so that one selling point seems to be enough for a, quite a lot of people to use it. And there's a long list of others, of these proton mail for email fathom, which we use for analytics on our website. There's this whole class of messaging apps like signal and telegram that have really gotten a lot of traction. And it's interesting to me, almost all of these that I just named, they're basically worse in every way than whatever they're competing with. Not always, I like fathom better than the Google analytics. And I think brave is nicely made mostly because it's just kind of a fork of Chrome, but in many cases they are about the same or worse, you know, using telegram to communicate with someone versus WhatsApp or SMS, for example, basically the same kind of experience, but that one benefit of some kind of privacy protection or some kind of surety that privacy is something that people have create the product care about is enough to get a lot of people to use it. I'm curious how you think about that? Are you motivated to use products that are privacy protecting versus trading that off against other things? Do you think that those kinds of products will always essentially just be a niche for the few people that care enough about it? Or do you think there's a future where that kind of focus would be something more mainstream? 

Speaker 1 (15:26):
Yeah. Well, that's not an easy question at him. Yeah. So I'd say first that I definitely use some of these privacy focused products. Two examples that I would give one is safari, which I use because it's faster. And I think it has better privacy capabilities than Chrome and other is Dr. Go, which for a long time, I've used almost exclusively instead of Google, I find that it works great and has a much better privacy story. So for me on the margin, I'm definitely inclined to look at the privacy angle and especially if things are comparable or if for some reason I care a lot about the privacy of that data, I will make the move. And, and I'm glad to see that we have these offerings and people can make choices like that to my mind. The bigger deal though, is the overall dynamics of the industry and what a lot of users end up choosing. 

Speaker 1 (16:12):
And yes, it's great if we have options for particularly privacy conscious or privacy sensitive people. And again, I'm very thankful for that. But if you think about this third reason that I mentioned a while ago, data concentration in large powerful entities, that's really determined by what most people do, right? So for that reason, I'm very interested in the overall dynamics of the industry and our governments and how those things interact. And to there, I would say that it's perhaps a more discouraging situation. I think there's things we can do and they're still passed out of this, but I think it'd be very easy to imagine a world where governments have access to all of our data, which by the way, you did a good survey there of some of the current privacy focused products, but a huge deal is access to TLS. And it's something we take for granted that you can go to HTTPS website, which basically all websites are now. 

Speaker 1 (16:59):
And at least that data won't be accessible to people online, unless there's some exceptional circumstances. And we take that for granted, but in my opinion, that was not a given at one point. It's my understanding that this public key cryptography was not a generally accessible technology. It was somewhat controlled by the government and with Netscape and commerce moving to the internet. Again, I'm not sure exactly how that story played out, but they eventually convinced the government to allow us to use that, to export it outside the country and so on. But I could absolutely imagine a world where that was not the case. Like if you can imagine, for example, that nine 11 happened before that stuff got out widely and web browsers, the government might have just said, you know what? We can't have people communicating securely at all. This is now banned. And then that would have been a very path dependent thing where we might've stayed in that situation. Yeah. I'm 

Speaker 2 (17:46):
Old enough that I lived through that process. I was in the computer world and in the industry for some of that, the clipper chip was actually a us government initiative at the time to create cryptography that had a built-in backdoor for government agencies to open. And there was also things like, yeah, up until the nineties, get in trouble here, recounting this all from memory 20 years on, someone's going to fact check me, hopefully you will. But at least the way I remember it was in the early nineties, it was the case that cryptography technologies were very rare. So I think it was up to 40 bit keys were allowed, but of course that's low enough that you can reasonably, I think even back then you can brute force crack them. So it wouldn't really be that viable for something like online commerce. And then I think, um, the, I don't know what you want to call it. 

Speaker 2 (18:32):
Technology cryptography folks, enthusiasts slash experts, uh, on the then sort of growing networked world were really, you know, arguing for why this technology could be really enabling. And there's good reason for governments in the us government in particular, to be cautious and treat it as a weapon because, you know, in many ways you can point to the allies winning world war two, you know, that was basically done through science. And one piece of that was maybe the atom bomb and the Manhattan project, but less dramatically was the cryptography story, right? The El insuring and the enigma machine. And the fact that one side could read the messages, another side couldn't and it was that asymmetric key cryptography. That was the technology that essentially allowed the allied communications to say, stay secret. So thinking of that as effectively having one that, or the greatest, certainly most destructive conflict in human history, and then be really cautious about who has access to that seemed quite reasonable. 

Speaker 2 (19:31):
Even, I don't know, 40 50 years on from said war. And yet the things that potentially enabled were so great. And of course now we live in that time where, as you said, SSL, and that little lock icon, the senior web browser makes it possible to have this huge, I mean, what would the world be like without online commerce in a pandemic age, right. Just to name one thing. So I'm glad we won that. Or let's say the people on the side of more access to encryption and privacy protecting technologies won that fight. But as you said, it definitely wasn't guaranteed 

Speaker 1 (20:05):
Now coming to muse specifically, historically, 

Speaker 2 (20:08):
We've had everything. In fact, we even say this when you first fire up the app or first login, which is, we basically say everything's stored on your device locally. It's private. That's important to quite a lot of people sometimes for very practical reasons. There are, for example, an attorney that has case notes in there, but in many cases, just coming back to that sense of privacy allows you to be freer. And if you feel like you can write stuff down and not feel like the NSA is looking over your shoulder, um, then that's just a better state to be in creatively. But now we're starting to move into much requested features that allow us to not be essentially in the iPads silo. So right now we already have a browser extension and an iPhone app there's for very simple capture into the iPad. But eventually we would like to imagine that you spans all your devices. 

Speaker 2 (20:55):
It's wherever you need it to be. So that's the multi-device side of things. Then it gets even more interesting what the multi-user side. And of course, we know that all these collaborative tools like Google docs and get hub and notion and Figma have really supercharged our work in the modern world. And certainly for remote teams like ours. Now, muse has a different use case, which is more about developing ideas than making these end work products. So what role exactly the collaboration and sharing side will play for us is not fully known yet. We'll explore that as we build this stuff out, but here we are confronting this thing where we on the team value privacy. We know that many of our users and customers value that a lot that makes you feel freer, but then we also know that being able to access your stuff from all your devices and share things with colleagues and friends is immensely powerful. So you've been leading the charge a little bit on thinking about the, particularly the technology sides of that. Trade-off where are we at? 

Speaker 1 (21:54):
Right? No. Well, let me start with the way this is done in almost all apps today. Note-taking productivity style apps. There is a central server that's run by the tool provider and that stores all of the user's data in a way that's accessible to that company. So you might have a table that's like documents and has the data for all documents for all users in it. And then when you fire up your app on a device, it talks to the server and says, give me the latest data on this document. And then it renders it on that device. And then if you share a document to another user that just becomes metadata and the database, it says for this document row, this user ID can access it. So in that user's device request the document, they can have authorization to get that data. All right. So when I make 

Speaker 2 (22:41):
A new blank document in notion or Google docs type in three characters, those three characters go into a record in a database somewhere in Google's servers. And that the cloud has, I believe they call it is precisely what makes it so easy to share, because if I want to send this to someone else that I need to take it off my device and put it on theirs, it's already in Google servers, Google essentially has ownership of that. I'm just accessing it through this client or front-end. And so giving one other person or some number of other people access through their client to front-end is a relatively straightforward operation, 

Speaker 1 (23:14):
Right? And notably with these modern cloud-based tools like Google docs and notion, and you typically don't even have the data locally. So if you type in this document, save exit and later you off the internet and you want to open up your document well, too bad, the data is not there. So that's the standard approach. And we remain open to doing that. For me, it was, it has a lot of benefits in terms of relative ease of implementation, of course, providing all of the features that you want, as well as things like backups in the case that the user loses all their devices or something. But we're also very interested, exploring a second way where you get the benefits that we've associated with cloud based collaboration, being able to access your data from any device, being able to add collaborators and collaborate in real time. All of those things without the tool provider, in this case, muse being able to read your data. So the way I pitched this is like signal meets Google docs. You have the security model of signal where data is end-to-end encrypted, and you're talking to your collaborators and only you, and they can read that data, but you have the rich documents, multimedia collaboration, multi-device synchronization that you would associate with Google docs. And that's quite a hard technology and product problem, but we are looking into it. You and I 

Speaker 2 (24:27):
Were both, co-authors on a paper called local first software, and this was much more research out word thinking technology of sort of removing the cloud from the equation completely, which is not what you were just describing there, but it does have some of these same elements of trying to make it so that it's less about what's out on these servers and more about what's on the individual users devices. I think we touched on encryption briefly, but I think, I mean, as we describe in that paper, we don't necessarily think that building a truly 100% local first application is really in reach for certainly for a small team like ours today. So it's really parts of that. We do think are achievable and other parts maybe are still a little bit more we're waiting for the technology to get good 

Speaker 1 (25:12):
Enough. Yeah. And I might say that I still think local first, at least as I understand it is possible for us. The thing that is less valuable and interesting is pure peer to peer. So there are some apps or technologies where if you and I are collaborating, we're sending packets back and forth directly to each other. And there's no server interposed, which has obvious potential security benefits, but it also gives you a certain resiliency against DTUs and other issues with a central server. And for me, having servers on the internet is not necessarily that big of a deal. And in many cases, there's just no way around it. For example, you need to talk to a central server to get apps on your iPad because apple requires it. And for me, the bigger deals are that central server being able to read all your data and you not being able to read and write your data if you're not connected to the central server. So the world that I imagine is one where you potentially have a server or even servers, but the servers are more like symmetric nodes. You know, they behave more like any other node, like your phone or your tablet and this not so much of a special super case. And then on the encryption 

Speaker 2 (26:21):
Side, you, you referenced signal. And I think one of the places they've been very influential is I want to say they started as this open whisper systems sort of security consultancy or something like that. And they not only made this secure messaging app, but they wrote a lot of articles and sort of publicize their approach. And that was something that's then been picked up by others, including WhatsApp and telegram. And I think the hard part in this is usually the key management, right? So this is asymmetric. Key cryptography basically relies on the person having this block of data someplace and no one else having that data, but that's tricky because that person has to keep track of it. Maybe it's a bit like a physical key. You lose a physical key to your house. You can intentionally go to a locksmith and they can crack it open for you the way that these digital keys work, if you lose it, that's it. You just can't get access to that data again. 

Speaker 1 (27:16):
Yeah. So there's a couple of things going on with an app, like signal, you might break it up into key management and cryptography. So on the cryptography side, this is like, okay, assume magically for a second, that everyone has keys and we know who has which keys. Then you need an algorithm for using those keys to encrypt the data. And there's been a lot of work on that in industry and in academia, it has been, I think, quite focused on the messaging use case. And one of the things that we're excited to do with muse, if we do pursue this end to end encryption path is making it more general case like let's encrypt data structures and documents and not make something that's very specific to messaging. Also in the case of signal, I do think they make some specific trade-offs that are more appropriate for the grade of security. 

Speaker 1 (27:57):
If you will, if they're looking for like signal needs to be resistant to powerful and nation state actors. And in order to do that, you need to make some specific trade-offs that maybe wouldn't be appropriate for a productivity tool. But anyways, yes, that's what happens on the crypto side. But then the key management side, that's a whole big challenge in many people will tell you that key management is actually the harder of those two issues, and there are different ways to do this. There's the fully distributed web of trust type model where you build up a model of who has, which keys based on a series of ideally in-person interactions. So, you know, you and I might meet in real life, we would exchange keys and we might also exchange information about other people that we have, and we would sign that information. And then over time you kind of create up this web of trust. 

Speaker 1 (28:36):
That's where the name comes from. That's kind of the most distributed, but least practical model, the most practical, but least secure model is just, you ask the server who has what keys and that's very convenient. You get all the benefits of a centralized server telling you exactly the right answer. The downside is the server can just lie and say, this is Adam's key. When in fact it's just a service key and it's three, nine Adam's data. And the thing that I'm most intrigued by, and that we're exploring a little bit with views is more of a middle ground, where you get some of the benefits from the centralized registry, but you also get some of the benefits from direct or decentralized verification, especially where you need it. So, one example of this in signal, I think they have this set up where you can look up people by phone number and the signal will essentially send your data to their devices, but you can also, when you're next to someone, you can verify each other's QR codes and that, and that lets you know that you've got you in fact verified this person, it gives you a stronger level of security. 

Speaker 1 (29:25):
So I think there's more things we can do along that vein. Another example from the zoom white paper, you know, zoom is working on and an encryption. They said for a long time, they have it. They don't really have it as we understand and an encryption because they've had this key management problem where everything was encrypted, like TLS is encrypted, but they control and administer all the keys. So they could just impersonate people if they wanted to. But they're trying to move to a proper model where you can in fact verify people if you desire to. And one of the things that they'll try is on your zoom video, you'll have a little code and when you're on the call or you can say to the person on the other side, you know, the security code is ABC 1, 2, 3, and that's very hard to impersonate in real time, obviously. So if you're correctly verifying the code on each side, you know, okay, this is not being tampered with. And there are other techniques that they're exploring too. But this basic idea of you kind of mix the benefits of a central registry for keys with more distributed ad hoc verification where you need it. 

Speaker 2 (30:14):
I think we'll be looking for the sweet spot there between trying to give some reasonable privacy protections, but not having a very difficult or very technical and demanding. For example, key management system. That's probably exactly what Zoom's grappling with right now. I think of the canonical example of inaccessible as something like PGP, right? And I've used the canoe PG for a number of years. I'm pretty good with it. I'm handy at the command line and even so it's just very easy to mess it up, get the wrong key, delete the key, encrypt the wrong thing. It's just very, very unforgiving even for a technical user. And so way, way out of reach for kind of more casual use an SSL is a great example. As you mentioned, it should be us websites is a great example where we did manage to find a way that was a middle ground of real solid encryption. That really does make a difference in terms of the things that enables, but it's very accessible. It does not require some kind of deep technical cryptography key management thing in order to get the benefits of it. 

Speaker 1 (31:24):
The last thing I might say on key management and crypto and user interface trade-offs and stuff, is that to us, it's very important that we design this all holistically. There is a lot of research, for example, on cryptography schemes that assume key management or on collaboration, product designs that assume the server can just read all the data. And in order for this to work with muse, we need all of the product design, the collaboration technology, the key management, the cryptography, the mental model for how people think about documents that all need to line up. And I think that's why this has proved quite challenging for us. It's not something that I think very people have grappled with, but I'm optimistic that if we can get all of those things to line up in the right way, I'll be a very powerful combination. Yeah. I'm 

Speaker 2 (32:06):
Potentially excited for that. Finding a new set of trade-offs. I feel like most tech industry products are essentially binary. You have either totally local old school program saves on your hard drive. Great. You know, no one else has it, but you just don't have any of those sharing and collaboration features or you have the fully in the cloud thing, which is just so incredibly useful. And yet you just know, okay, I'm just giving up every single keystroke by type into this. I know that Google engineers and Google machine learning algorithms and the NSA and anyone in the future that acquire this data for essentially an infinite amount of time, I'm just giving up and saying they have full access. It's the trust us model vendor model. And I'm excited that we can potentially find a different set of trade-offs a different sweet spot, a different place to be that isn't one of those two extremes. 

Speaker 2 (32:57):
Definitely. Yeah. And normally when the topic of privacy comes up in the context of the tech industry, generally one of the key things is people are talking about my data and I think we've almost been entirely talking about what I would call content. And so maybe content privacy. I make a document. I write a note, I record a video, that's my content. And I want to know that I am the only person me and people I have chosen to share are the only people that have access to that. But the other category, and maybe even a more common one to be in these discussions is more call it analytics, or you can call it telemetry data. And it's a really interesting question when you do frame it as data ownership, if there's something like a motion sensor, for example, a smart home kind of motion sensor that is logging entry and exit to a location. 

Speaker 2 (33:45):
If I put that in my home and I'm logging that into a computer, I control, I feel, feels pretty clear to me that that data about the comings and goings in my home as mine. But if it's in another building, say a public building and I walk in and my arrival is recorded. And of course, you know, cameras there, your faces, you know, in the data, is that mine? Well, probably not. But then it kind of, that is part of the discussion. It's like, well, wait a minute, you're kind of recording me or tracking me. I feel like that belongs to me somehow or you're invading my privacy and it's, but then you're, you're in that other person's building, you know, the building is a public place or not something that belongs to you. So it's a funny thing to discuss in 

Speaker 1 (34:24):
A way. Yeah. Okay. So to unpack that, I think there's actually three different things going on there. One is that classic analytics data and the example that I'm like it for that is say you have a web app and it's indicating how often this user uses certain features. Like, do you use the export feature? Do you use the printed PDF feature? Things like that. That's what I would think of as analytics. Then there's the PII, the personally identifying information. This is information that ties some abstract user to you as a real person. So typically emails, that's assigned names plus addresses, phone numbers. These are things that take a add fact account, you know, user ID, whatever, and tie them to Adam Wiggins. And then there's this third issue of data in public places. I think that's another huge challenge. And to my mind, those are three different, quite hard issues. Mm. 

Speaker 2 (35:11):
And notably the, you know, I mentioned GDPR and passing before cookie warnings are this huge thing in Europe where basically almost any say you go to pops up this morning and it's kind of regulatory things gone wrong, where they were trying, I think quite reasonably to say like, if a site is going to track you in some way that they should seek your consent, but now essentially most websites just do a kind of blanket consent seeking because most websites, some kind of cookie, but the detail of it actually is that it only matters exactly. As you said, if it's personally identifying some way, if it's sort of tracking you around. So there are a lot of cookies that you might set that are more of kind of anonymous or more kind of general telemetry, but are not about me specifically. And then, so for example, the muse website does have analytics. 

Speaker 2 (35:59):
You can see that if you do view source, but it does not have a cookie warning. And that's because the type of analytics that we use is essentially anonymous. It doesn't track you around it. Doesn't connect to what you're doing somewhere else. It just counts essentially hits to our site, which is very useful to have. It's nice to know, especially with refers or whatever, it's, you know, if there's suddenly a thousand new users pop in, wow, where'd these folks come from? Oh, I see we got linked on some high profile site. We can see that in our analytics, it's very useful to have for our business. 

Speaker 1 (36:28):
Yeah. And GDPR, by the way, is I think a good example of the importance of systems thinking. I think the failure of that legislation and I, I mean, perhaps sounds blunt, but I think that's the correct assessment was due to not thinking about it as a systems problem, where you have to deal with the realities of what are people actually gonna do? What are businesses actually gonna do? What are the capabilities or not capabilities of the government? Things like that. I do think that the differentiation between analytics and PII is important and good to my mind, those are just very different beats as well as being different versus content. This is something that we've kind of caught up in discussions with users and, you know, sometimes a privacy fundamentalist who says, you know, everyone has a right to privacy, no data should be ever transmitted over the network without my explicit permission, you know, maybe, but the reality is it's hugely valuable information that for most people has relatively low costs in terms of their privacy cost, if you will. So it's not surprising that people end up sort of making that exchange. It's much easier to, and therefore cheaper to offer software. If you have access to this analytics data, whereas it's a relatively low cost to individuals in terms of their privacy. I do think that PII and content stuff is much more tricky and PII is also slippery because you can collect the data. That's like quote, unquote anonymous, but it's actually very easy to de-anonymize if you collect enough of their screen size S version browser version. Yeah. Yeah. 

Speaker 2 (37:55):
The browser fingerprinting stuff, which is for a little while I was following kind of the Tor browser world of things, which is another one of these, well, that's even a step further on the privacy focused products. I think that's very interesting stuff that that team is doing, trying to make truly anonymous web browsing. And one of the things they have had to face up against is the browser fingerprinting, which is exactly what you said, which is sort of knowing the exact resolution of the screen. And you know, what version of the operating system they're on. He tied that together with some other data, some timestamps and things you can work backwards from there to pinpoint someone at least that it's the same person repeatedly, surprisingly well, 

Speaker 1 (38:34):
Yeah. I also think that the PII information can be separated from contact information and muse this something interesting. Here, we do require an email to use the app because we need to be able to communicate with you for various reasons, but there's no requirement that the email is like your only email or your canonical email or that it matches any other emails. So a lot of people just put in their default personal email, but a lot of people will create an email that's specific to muse kind of like a use specific inbox or those whose like a burner email that has no connection, no identifiable connection to their real identity. And again, this is an example of where you can tease these things apart. You can separate out PII from ability to contact someone from analytics information, from content information 

Speaker 2 (39:16):
And speaking to muse on the kind of analytics side of things. I mentioned our website, but the product itself, the iPad does report back analytics on usage. And that is for improving the product. Huge one for example is crash reports. So when I've had, oh, S 14 came out a little while back, we embarrassed to say had a very Rocky patch for a couple of weeks of crashing us. And that was partially changes in the OS. Partially was problems on our side. We eventually sorted it all out. I'm happy to say, but if we didn't have reliable crash reports to be able to see, first of all, that there is a problem. And secondly, to try to hone in on what that problem is, and then once we've fixed it, you know, we roll out a new release, has the rate of this particular kind of crash gone down that has a big impact for our ability to not be blind or trying to improve the product. 

Speaker 2 (40:11):
But it also includes things like just features. So a little while back Leonard was redesigning the action bar, which is what comes on the screen. When you tap a blank space and you get the little couple of buttons down at the bottom, and we were really pondering whether the undo redo were worth, including they took up a lot of space and most people use the gesture, or you can use the keyboard shortcut in the case, we had the keyboard. And so we thought, well, it was really worth the screen real estate. This takes up. And we could actually go ask this question of what percentage of people are using the buttons or what percentage of the time versus using a gesture or a keyboard shortcut. And I forgot where it came out. I think it was like 15% or something like that. 10 or 15% of undoing were from the action bar button. And that made us say, you know, that's just enough. I think it's worth keeping, we'll make them a little smaller maybe to represent that so we can make product choices. And of course there is ways to, I don't know, go ask people. Of course, there's, you know, you should be doing that. And occasionally we get to observe people using the product and so on, but the ability to go and get real data about those kinds of questions, they really help us to improve the product. And so it can become a better product faster. 

Speaker 1 (41:17):
Yeah, totally. And again, this seems to me like an imminently reasonable and good trade-off for both sides involved. If we had to develop muse use without access to this information, and it would be much, much harder and it would be worse a product and maybe it would cost twice as much or maybe be half as good. But is that worth this very marginal amount of privacy in terms of analytics information? I don't think so. No. I've discussed a lot of this in terms of favoring, a sense of tradeoffs and opt-in decisions over fundamentalism in any direction. I do think a huge issue with privacy again in the electronic realm is it's very hard to understand what's going on, you know, muse. I like to think we try to be a good actor. We try to do reasonable things and nothing nefarious, but it's basically very easy to do really bad stuff in terms of privacy, especially on non web platforms. And to my mind that that's actually a big technology gap, you know, how do we empower users to actually make these trade-offs instead of just having to throw up their hands. And there have been some movement on this. I think apple is coming out with some additional required information from developers soon about what information you collect and how you use it and so on. And that's certainly a step, but I suspect that as much, much more to do here. 

Speaker 2 (42:31):
And in general, I think I, and most people would point to apple is one of the best actors in terms of moving the ball forward on what users can expect privacy wise. And I think this stems out from iOS from the beginning was a very securely designed operating system, much more than the classic desktop and server operating systems that that came before. And they've continued to do that. I think of something like the onscreen notifier, when and hap accesses your clipboard. Yup. When they rolled that out a little while back then suddenly you saw these slightly shady things that many apps were doing, including I think Tik TOK pulling from the clipboard on every single keystroke, for example, and you need that. You need a clipboard that can move between apps and apps that are going to do interesting things need to access the clipboard, but finding ways to try to surface that so that people who are not acting in good faith, not using the operating systems capabilities for benefit of the users, but for their own benefit or at the very least just being deceitful perhaps about what the user expectations are versus what they're actually doing. 

Speaker 2 (43:32):
So certainly got to give props, tap on that. They're not perfect, but they're definitely one of the best players I think, in our industry, certainly at that scale. Oh yeah. 

Speaker 1 (43:40):
I mean, lots of good things going on there and it helps when you have control over the platform because you can manage access to things like the camera roll and the microphone. And so on. I guess my point here is that I think there's just a huge gap remaining. Especially as you look at the meat of the data and the network connections, you know, apps, they can basically talk to anywhere on the internet they want and they can do whatever they want with data that you input into the app. So a good example of this is perhaps you have an app for composing a message the app can. And in fact, many of them do just send every keystroke that you type, regardless of whether you hit send or when you hit send. So you might not realize it, but when you're drafting a message, whoever is running this app has a copy of that draft forever. Even if you later decide, oh, that's actually not a good idea to send that to the backspace, Hey, that's really questionable. But also it's really tough to manage against like, what would the interface be that prevents apps from doing that, or even alerts users that it's happening? It's hard. 

Speaker 2 (44:32):
Yeah. I have a tendency to type messages and progress. If there are anything more significant than just a, you know, a few words for a quick reply into my local text editor, very programmer type thing to do. But first of all, I liked the better editing capabilities, but it's also the sense of knowing that it's not a cloud connected thing that it's truly, you know, when I hit that close button, maybe it prompts me of whether I want to save, but you compare to the cloud where anything you've ever put into it is just always saved instantly, which by and large is a huge win for users like unsaved documents or things you forgot to save. And then your computer crashes and, and whatever, that has been a massive source of user frustration for a very long time. And this modern era of mobile apps and cloud that don't really have a concept of needing to save things. And just everything you type in is ever saved is mostly a big user experience. When, but for me personally, yeah, when I'm composing a message, I like to know that it's in this ephemeral place and that if I decide, eh, this isn't quite right or whatever, I just deleted and I know it never went anywhere. 

Speaker 1 (45:36):
Yeah, sure. That makes a lot of sense. Oh, and by the way, on mobile, another huge way that the mobile platforms achieve security is just banning huge categories of stuff, especially arbitrary code execution, plugins, extensions, and these are capabilities on the one hand are incredibly powerful. You can argue that they're basically required for getting a lot of the power user workflows that you see on desktops, but they would be super gnarly to sandbox. And to be clear, I'm not saying that's a wrong decision or that apple or anyone else hasn't done enough for that. They're making bad calls here. It's more to point out that I think it's an incredibly important research problem. Is there a way to get the benefits of third-party plugins and security at the same time? Right now it's very much one or the other in the same way that it's possible to get collaboration and end-to-end encryption at the same time. You know, it's an open research question to see if you can figure out how to do that 

Speaker 2 (46:26):
A little earlier. You mentioned the term privacy fundamentalism. Yeah. And I like this concept of trying to just better understand how much does privacy matter to people? How much does it really matter? And for us, you know, from a business perspective, we can sit here. And in fact, we do talk about big philosophical things that we believe in regarding privacy and what the technology industry should do, and the things our society are going to be grappling with having to do with this. But we're a small business. We need to do things that make sense practically for us and places. We invest effort, time, energy, money are places. You know, that is zero sum. We could be building out other features. And if we're looking into, and then encryption to make a signal style thing for creative professionals to share their little notebooks, you know, that comes at the expense of other things. 

Speaker 2 (47:11):
How important is it really to our potential customers? And one of the pieces of priority, I was just kind of looking at when we were thinking about this episode was essentially a survey of people's attitudes about privacy. And in this case, I think it was like an internet of things kind of category. So I think that so in the of smart home or something like that, but I liked that they broke things out into three categories. In terms of people's attitudes, there was the privacy fundamentalists, which you described, which were people who would trade off almost anything for the privacy aspect of things. And then you had another category, which was also a small group, but still a significant one, which they called privacy unconcerned, which he said, they just said, who cares? Nothing I do is that interesting. Google has all my data. Anyways, what difference does it make? 

Speaker 2 (47:59):
I don't care. But then the biggest category by far is what they call the privacy pragmatists. That's certainly the category I put myself in, which is, this is something I care about. I think it is important. It has impacted my life in direct ways in the past. And I do see it as a big, an important topic for our industry, for our society going forward, but I'm not willing to trade off everything for it. There's a bunch of other things that I care about in terms of the utility of products that I'm using. And so finding that balance, I think at least when I put it in this frame, I'm going to make a wager that I think a lot of muse users, both current and future are privacy. Pragmatists. 

Speaker 1 (48:34):
Yeah. I think that's right. And I agree on privacy fundamentalism. I think that can actually mean two things. I think it can mean that one as an individual has very high standards for privacy and I think that's totally fine for an individual to say. And I think for some people it's absolutely the right decision. For example, if you are acting as an informant, if you're doing something that the government doesn't like so on and so forth, right. As to correct trade-off to make the thing that I am not as sympathetic towards is the sense of privacy fundamentalism. I sort of, the entire system should be subject to it. And this is where I come back to this idea of privacy as a fundamental human, right. That sounds very appealing. But on the flip side, it's saying that no, you should not have the ability to choose to be a privacy pragmatist. 

Speaker 1 (49:17):
You shouldn't be able to use software that takes a different set of trade-offs with respect to privacy. And that's something that I'm not very sympathetic to. And I would furthermore say that I also think it's valid that you want to live in a society where many, or all people choose to be privacy fundamentalists, or choose to have very strong standards for privacy. But I think we need to recognize that's an enormous amount of work. It will cost many, many millions of dollars to develop and operate that software. And it will require perhaps trade off in terms of our day-to-day experience with the software. And if you kind of ignore that cost, you're fooling yourself and it's being intellectually dishonest and ended up not achieving that one other possible angle on privacy. Fundamentalism is again, going back to governments, there's a possibility that a loss of privacy is a one way very destructive ratchet. And that for that reason, you want to be very careful. This is a sense that once a government has access to data, they might be extremely reluctant to forego that grip. And over time, they're going to tend to get access to more and more through various means. And if you see the end game is being bad, some people do. And some people don't, but if you see that end game as being bad, then you'll want to resist by basically any means possible that progression of that ratchet 

Speaker 2 (50:37):
Well talking about governments makes me want to recount my personal journey into thinking about privacy in a broad way. I think for a lot of Americans, it was the case that the Edward Snowden incident, which revealed how much kind of digital surveillance the U S government was doing on its citizens was a bit of a wake up call. Now for me, I think I'd always had the vague sense that this is something important. And I know digital technology is going to change the game, but I don't think I'd given it deep thought. And by just a coincidence of life, this all was sort of unfolding, right? As I was moving to Germany and I watched the startup I was working for at the time Berlin, they just organized a little outing basically to see this documentary citizen for in the theater, which was a very interesting experience where they followed Edward Snowden around with the camera. 

Speaker 2 (51:29):
And this was, you know, before the news had broken, essentially. And so you see all that unfold and you see the crazy lengths, he goes to, you know, the tails, Lennox distribution, and putting a blanket over his head as a actually very reasonable security precaution when he's typing in his password. That sort of thing, very interesting film. But that in turn led to me having kind of a lot of conversations with my colleagues, there are many of whom grew up in Germany and for them, it was very much in the recent past the east German Staci, which was kind of a secret police that had, I think at least as far as we know, is the most extensive government monitoring of its citizens to date some crazy thing. Like one third of the entire east German population was a Staci informer. And when these records were revealed, when the Berlin wall came down in 1989, and these records were revealed to the population and people realized how much had been tracked and largely using at the time, the new technologies of things like small microphones and wire taps and, you know, recording things tape and stuff like that, to just the extent of it, just shocked people had no idea that such a thing could be possible, actually another great film worth checking out a fiction. 

Speaker 2 (52:40):
But I think captures this well. It's called the life of others, German film that sort of depicts a fictional Shazi officer and what they're doing and they're monitoring. But as I spoke to these folks who this stuff is in their living memory, they grew up when this was happening, right. This is only now 31 years ago at the time 24 years ago. And they said, you know, we know what it looks like. Maybe I had almost a little bit of an innocence. You might say in so far as being this American where I guess I basically just feel like most of the time, you know, government can be bureaucratic. It can sometimes be incompetent, but ultimately most of the people that work in government and the systems that are in place are basically there to serve the citizenry. And yeah, there's a lot of trade-offs about law enforcement getting access to wiretaps and stuff like that. 

Speaker 2 (53:27):
But ultimately they want to do that to catch the bad guys, keep us safe, all that kind of stuff. And speaking to the used German folks where they said, you know, no we've seen what it looks like to have a society where a government so heavily monitors its citizens in the name of protecting that society, right. Everyone that worked, you know, in this state surveillance apparatus believed they were doing something really good. They were keeping their home safe. And I'm not saying I necessarily converted to seeing the world that way. I do see it as a series of trade-offs and yet that was a powerful experience for me. And I think has influenced strongly how I see this topic as it unfolds in the technology world. 

Speaker 1 (54:07):
Yeah, absolutely. And I think it shows how dangerous it might be to just assume or hope that everything will go well. If we concentrate an enormous amount of data in a very legible way in one or a few powerful entities, 

Speaker 2 (54:23):
Yes. Silicon valley has maybe already grappled with this a little bit, which is you have a bunch of young optimistic people building powerful new communications and other kinds of digital technology. And they tend to think about the positive case. You know, I think people that get into tech tend to be optimists. They tend to think of technology as a force for good. And they're not thinking about the ways that it can cause harm. It's, uh, you know, technology is neutral and can be used as a weapon can be used for harm, can decay the things that a society holds dear. And I don't think that's a reason to fear it or to kind of have a knee jerk reaction. But I do think there is a clear-eyed sense of, okay, as we open up brand new ways to do all kinds of things with our information life, thanks to these digital technologies. 

Speaker 2 (55:11):
What's that going to mean? And not to say that we can fully know or fully predict what the impact of this stuff is, but I think being mindful and having some caution as we go, that certainly goes for you and I who work on new products where we're trying to bring new capabilities into people's lives, what are the risks? What are the downsides? And certainly the privacy product issues that we're grappling with right now are very much in that category for me, for sure. Well, if any of our listeners out there have feedback, for example, we'd love to hear how you think about privacy and digital tools. Go ahead and reach out to us at muse app HQ on Twitter, or you can email us@helloandmuseapp.com. We always like to hear your comments and ideas for future episodes and mark. I certainly hope you'll keep us in the loop about how you're thinking about these trade-offs on the technology side, on the product side and on the philosophical side, as we continue to explore 

Speaker 3 (56:06):
What we can do with the collaboration and multi-device capabilities of news. Absolutely. Really looking forward to this, right. Have a good one. [inaudible].

